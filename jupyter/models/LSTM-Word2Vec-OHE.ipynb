{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv(\"../../data/cleaned/out.csv\")\n",
    "\n",
    "# Text preprocessing\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'\\W', ' ', text)  # Remove special characters\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
    "    return text\n",
    "\n",
    "df['text'] = df['text'].apply(preprocess_text)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/juan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "df['text'] = df['text'].apply(remove_stopwords)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.0002824   0.50857365 -1.5845125  -1.0070028   1.0680755  -0.02567383\n",
      "  0.24009354  3.3515723  -1.6856357  -0.94262326  0.36953592 -2.4019828\n",
      "  0.87575024  1.4228108  -0.35064992  0.7064087   1.7947127  -0.78772587\n",
      " -1.4907619  -0.7764842   1.3168302   1.5367247   2.0797865   0.8773612\n",
      " -0.80978525  1.8394427  -1.0109826   0.08838938 -0.11758807  0.27711758\n",
      " -0.19220321  0.03485214 -1.1287682  -0.35237545  0.0869664   2.6283176\n",
      "  0.3283814  -1.8839879   0.25753117 -0.09678795  1.8077937  -0.32725492\n",
      "  0.5822203   0.15976019  1.3051248  -0.18149598 -1.3326164  -1.8458551\n",
      "  1.5937154   0.07723075]\n"
     ]
    }
   ],
   "source": [
    "tokenized_texts = [text.split() for text in df['text']]\n",
    "word2vec = Word2Vec(tokenized_texts, vector_size=50, window=5, min_count=1)\n",
    "print(word2vec.wv['good'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juan/anaconda3/envs/SentAnalysis/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3464: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n"
     ]
    }
   ],
   "source": [
    "def text_to_vector(text):\n",
    "    words = text.split()\n",
    "    word_vectors = [word2vec.wv[word] for word in words]\n",
    "    return np.mean(word_vectors, axis=0)\n",
    "\n",
    "df['vector'] = df['text'].apply(text_to_vector)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juan/anaconda3/envs/SentAnalysis/lib/python3.8/site-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 1., 0., 0.],\n       [0., 0., 0., ..., 1., 0., 0.],\n       ...,\n       [0., 1., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [1., 0., 0., ..., 0., 0., 0.]])"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = OneHotEncoder(sparse=False)\n",
    "encoded_labels = encoder.fit_transform(df['label'].values.reshape(-1, 1))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
