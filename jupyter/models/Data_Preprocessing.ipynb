{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../../data/text/cleaned/final.csv'\n",
    "DATA_OUTPUT_PATH = '../../data/text/cleaned/preprocessed/final.pkl'\n",
    "CHUNK_SIZE = 10**6\n",
    "COLUMNS = ['text','priority']\n",
    "words = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRE PROCESAMIENTO DE DATOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocesamiento\n",
    "Es necesario remover de nuestros datos información irrelevante como etiquetas, puntución, números y caracteres especiales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "TAG_RE = re.compile(r'@[^> ]+')\n",
    "\n",
    "def remove_at_sign(sentence: str):\n",
    "    '''\n",
    "    Replaces '@' from and input string for an empty space\n",
    "    :param sentence: String that contains @\n",
    "    :return: sentence without @\n",
    "    '''\n",
    "\n",
    "    return TAG_RE.sub('', sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langid\n",
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "def translate_sentence(sentence: str):\n",
    "    \"\"\"\n",
    "    Translate a sentence to english if it's in different language.\n",
    "    :param sentence: The string/sentence to translate\n",
    "    :return: The original sentence in english\n",
    "    \"\"\"\n",
    "    lang = langid.classify(sentence)[0]\n",
    "    if lang != 'en':\n",
    "        sentence = GoogleTranslator(source='auto').translate(sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T12:57:06.283719Z",
     "start_time": "2023-05-14T12:57:00.088199Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def preprocess_text(sentence: str):\n",
    "    '''\n",
    "    Cleans up a sentence leaving only 2 or more non-stopwords composed of upper and lowercase\n",
    "    :param sentence: String to be cleaned\n",
    "    :return: sentence without numbers, special chars and long stopwords\n",
    "    '''\n",
    "\n",
    "    cleaned_sentence = sentence.lower()\n",
    "    cleaned_sentence = remove_at_sign(cleaned_sentence)\n",
    "    cleaned_sentence = re.sub('[^a-zA-Z]', ' ', cleaned_sentence)\n",
    "    cleaned_sentence = re.sub('\\s+[a-zA-Z]\\s', ' ', cleaned_sentence)\n",
    "    cleaned_sentence = re.sub('\\s+', ' ', cleaned_sentence)\n",
    "\n",
    "    #Translate\n",
    "    cleaned_sentence = translate_sentence(cleaned_sentence)\n",
    "\n",
    "    #Removal of stopwords\n",
    "    pattern = re.compile(r'\\b(' + r'|'.join(stopwords.words('english')) + r')\\b\\s')\n",
    "    cleaned_sentence = pattern.sub('', cleaned_sentence)\n",
    "\n",
    "    return cleaned_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def add_to_dict(dictionary, filename):\n",
    "    \"\"\"\n",
    "    Agrega elementos a un diccionario desde un archivo.\n",
    "\n",
    "    :param dictionary: Diccionario al que se agregarán los elementos.\n",
    "    :type dictionary: dict\n",
    "    :param filename: Nombre del archivo.\n",
    "    :type filename: str\n",
    "    \"\"\"\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            line = line.split(' ')\n",
    "\n",
    "            try:\n",
    "                dictionary[line[0]] = np.array(line[1:], dtype=float)\n",
    "            except:\n",
    "                continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenización y Lematización\n",
    "Una vez cargada la información de los tokens de GloVe se procede a tokenizar y lematizar cada\n",
    "una de las oraciones en nuestro set de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def sentence_to_token_list(sentence: str):\n",
    "     \"\"\"\n",
    "    Convierte una oración en una lista de tokens útiles.\n",
    "\n",
    "    :param sentence: Oración a procesar.\n",
    "    :type sentence: str\n",
    "    :return: Lista de tokens útiles.\n",
    "    :rtype: list[str]\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    useful_tokens = [token for token in lemmatized_tokens if token in words]\n",
    "\n",
    "    return  useful_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el token anterior, el cual sabemos se puede representar por medio de uno de los tokens almacenados en `words`, entonces pasamos a la representación de estos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_words_vectors(sentence: str, word_dict=words):\n",
    "    \"\"\"\n",
    "    Convierte una oración en vectores de palabras.\n",
    "\n",
    "    :param sentence: Oración a procesar.\n",
    "    :type sentence: str\n",
    "    :param word_dict: Diccionario de palabras con vectores asociados, por defecto es `words`.\n",
    "    :type word_dict: dict, optional\n",
    "    :return: Array de vectores de palabras.\n",
    "    :rtype: numpy.ndarray\n",
    "    \"\"\"\n",
    "    processed_tokens = sentence_to_token_list(sentence)\n",
    "\n",
    "    vectors = []\n",
    "    for token in processed_tokens:\n",
    "        if token in word_dict:\n",
    "            token_vector = word_dict[token]\n",
    "            vectors.append(token_vector)\n",
    "\n",
    "    return np.array(vectors, dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejecucion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data:pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Procesa los datos en un DataFrame.\n",
    "\n",
    "    :param data: DataFrame que contiene los datos a procesar.\n",
    "    :type data: pd.DataFrame\n",
    "    :return: Columna 'text' del DataFrame procesada como vectores de palabras.\n",
    "    :rtype: pandas.Series\n",
    "    \"\"\"\n",
    "    data['text'] = data['text'].apply(preprocess_text)\n",
    "    return data['text'].apply(lambda sentence: sentence_to_words_vectors(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "add_to_dict(words, './GloVe/glove.6B/glove.6B.50d.txt')\n",
    "\n",
    "first = True\n",
    "count = 1\n",
    "data_array = []\n",
    "\n",
    "for chunk in pd.read_csv(DATA_PATH, chunksize=10**5,nrows=10**6):  \n",
    "    print(\"PROCESSING \" + str(count))\n",
    "    data = process_data(chunk)\n",
    "    data_array.append(data)\n",
    "\n",
    "    count = count +1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
