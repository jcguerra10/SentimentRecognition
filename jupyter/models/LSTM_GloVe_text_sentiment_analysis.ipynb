{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# LSTM-GloVe Model for Sentiment Analysis in text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "En este notebook se describe el proceso de carga, preprocesamiento, embedding, construcción y entrenamiento de un modelo que emplea LSTM y GloVe\n",
    "para el set de datos de texto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Carga de Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "A continuación se cargará el dataset unificado que se construyó en etapas anteriores (Ver `data_join.ipynb` y `NLP_tasks.ipynb`) que cuenta con cas 66000 registros de texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T00:58:40.087569Z",
     "start_time": "2023-05-01T00:58:37.798720Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('../../data/cleaned/out.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T00:58:53.413010Z",
     "start_time": "2023-05-01T00:58:53.381796Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Preprocesamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Emociones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Se realizará el método de one_hot_encoding para nuestra varible de salida del modelo: Las 7 emociones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T00:58:57.695393Z",
     "start_time": "2023-05-01T00:58:57.633498Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Emociones\n",
    "emotions = data['label'].unique()\n",
    "print(emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T00:58:59.181639Z",
     "start_time": "2023-05-01T00:58:59.063532Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = pd.get_dummies(data.label)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Es necesario remover de nuestros datos información irrelevante como etiquetas, puntución, números y caracteres especiales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T00:59:03.264094Z",
     "start_time": "2023-05-01T00:59:03.247716Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "data['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T00:59:06.689496Z",
     "start_time": "2023-05-01T00:59:06.645635Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TAG_RE = re.compile(r'@[^> ]+')\n",
    "\n",
    "def remove_at_sign(sentence: str):\n",
    "    '''\n",
    "    Replaces '@' from and input string for an empty space\n",
    "    :param sentence: String that contains @\n",
    "    :return: sentence without @\n",
    "    '''\n",
    "\n",
    "    return TAG_RE.sub('', sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T00:59:08.978903Z",
     "start_time": "2023-05-01T00:59:08.954464Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "remove_at_sign(data['text'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T00:59:19.178655Z",
     "start_time": "2023-05-01T00:59:11.277560Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T00:59:19.191669Z",
     "start_time": "2023-05-01T00:59:19.178655Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def preprocess_text(sentence: str):\n",
    "    '''\n",
    "    Cleans up a sentence leaving only 2 or more non-stopsentences composed of upper and lowercase\n",
    "    :param sentence: String to be cleaned\n",
    "    :return: sentence without numbers, special chars and long stopsentences\n",
    "    '''\n",
    "\n",
    "    cleaned_sentence = sentence.lower()\n",
    "    cleaned_sentence = remove_at_sign(cleaned_sentence)\n",
    "    cleaned_sentence = re.sub('[^a-zA-Z]', ' ', cleaned_sentence)\n",
    "    cleaned_sentence = re.sub('\\s+[a-zA-Z]\\s', ' ', cleaned_sentence)\n",
    "    cleaned_sentence = re.sub('\\s+', ' ', cleaned_sentence)\n",
    "\n",
    "    #Removal of stopsentences\n",
    "    pattern = re.compile(r'\\b(' + r'|'.join(stopwords.words('english')) + r')\\b\\s')\n",
    "    cleaned_sentence = pattern.sub('', cleaned_sentence)\n",
    "\n",
    "    return cleaned_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T00:59:19.300991Z",
     "start_time": "2023-05-01T00:59:19.196670Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preprocess_text(data['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T00:59:55.875793Z",
     "start_time": "2023-05-01T00:59:19.229466Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from copy import  deepcopy\n",
    "\n",
    "cleaned_data = deepcopy(data)\n",
    "cleaned_data['text'] = cleaned_data['text'].apply(preprocess_text)\n",
    "cleaned_data['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Para el proceso de embedding se usarán los datos de un modelo de embedding como lo es GloVe.\n",
    "La información dicho modelo será cargada dentro de `words`. Cada uno de los tokens de GloVe que\n",
    "se usará tiene una dimensión de 50. En caso de que"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T01:00:05.684493Z",
     "start_time": "2023-05-01T00:59:55.881789Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "words = {}\n",
    "\n",
    "def add_to_dict(dictionary, filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            line = line.split(' ')\n",
    "\n",
    "            try:\n",
    "                dictionary[line[0]] = np.array(line[1:], dtype=float)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "add_to_dict(words, './GloVe/glove.6B/glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T01:00:07.113715Z",
     "start_time": "2023-05-01T01:00:05.690516Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Tokenización y Lematización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Una vez cargada la información de los tokens de GloVe se procede a tokenizar y lematizar cada\n",
    "una de las oraciones en nuestro set de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T01:00:07.296718Z",
     "start_time": "2023-05-01T01:00:07.075667Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Ejemplo de cómo se debería de tokenizar y lematizar una oración:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T01:00:09.441765Z",
     "start_time": "2023-05-01T01:00:07.267716Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "sample = preprocess_text(cleaned_data['text'][0])\n",
    "token_sample = tokenizer.tokenize(sample)\n",
    "lemma_sample = [lemmatizer.lemmatize(token) for token in token_sample]\n",
    "lemma_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "A continuación se define una función para la tokenización y lematización. Adicionalmente, el token final que se entrega únicamente contiene palabras definidas en `words`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T01:00:09.516767Z",
     "start_time": "2023-05-01T01:00:09.446766Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sentence_to_token_list(sentence: str):\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    useful_tokens = [token for token in lemmatized_tokens if token in words]\n",
    "\n",
    "    return  useful_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T01:00:09.531766Z",
     "start_time": "2023-05-01T01:00:09.461765Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentence_to_token_list(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Con el token anterior, el cual sabemos se puede representar por medio de uno de los tokens almacenados en `words`, entonces pasamos a la representación de estos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T01:00:09.532765Z",
     "start_time": "2023-05-01T01:00:09.477767Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sentence_to_words_vectors(sentence: str, word_dict=words):\n",
    "    processed_tokens = sentence_to_token_list(sentence)\n",
    "\n",
    "    vectors = []\n",
    "    for token in processed_tokens:\n",
    "        if token in word_dict:\n",
    "            token_vector = word_dict[token]\n",
    "            vectors.append(token_vector)\n",
    "\n",
    "    return np.array(vectors, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T01:00:09.532765Z",
     "start_time": "2023-05-01T01:00:09.494768Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentence_to_words_vectors(sample).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T01:00:09.545765Z",
     "start_time": "2023-05-01T01:00:09.515769Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentence_to_words_vectors(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T01:07:08.842774Z",
     "start_time": "2023-05-01T01:06:58.574974Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Se obtiene nuestro conjunto de datos X\n",
    "X = cleaned_data['text'].apply(lambda sentence: sentence_to_words_vectors(sentence))\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Dado que las matrices de vectores de cada oración tienen un número diferente de filas debido a que cada oración cuenta con un número diferente de palabas. Es necesario identificar el tamaño máximo de los textos que se tienen para su \"estandarización\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T01:08:30.301341Z",
     "start_time": "2023-05-01T01:08:30.182293Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temporal = deepcopy(X)\n",
    "temporal['len'] = temporal.apply(np.shape)\n",
    "\n",
    "MAX_LEN = max(temporal['len'])[0]\n",
    "MAX_LEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Dado que el tamaño máximo es 35, entonces se llevarán todas las matrices a la forma `(35, 50)`. Los valores faltantes para cada vector serán 0s en su inicio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T01:11:58.362852Z",
     "start_time": "2023-05-01T01:11:53.413029Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "X_ = tf.keras.utils.pad_sequences(X, maxlen=MAX_LEN, dtype='float32')\n",
    "# X_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T01:12:03.087480Z",
     "start_time": "2023-05-01T01:12:03.061449Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T01:12:06.580315Z",
     "start_time": "2023-05-01T01:12:06.539900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T01:12:09.081556Z",
     "start_time": "2023-05-01T01:12:09.056503Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## División Entrenamiento-Validación-Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T01:12:40.748468Z",
     "start_time": "2023-05-01T01:12:39.779816Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test =  train_test_split(X_, y, test_size=0.2, random_state=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.layers import Embedding, Lambda, LSTM, Flatten, Dense, Input, Dropout, Bidirectional, GlobalMaxPooling1D\n",
    "from keras.optimizers import Adam, RMSprop, SGD\n",
    "from kerastuner import RandomSearch, HyperParameters\n",
    "\n",
    "def build_model(hp):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(MAX_LEN, 50)))\n",
    "\n",
    "    # Hiperparámetros para LSTM 1\n",
    "    lstm_units = hp.Int(\"lstm_units_1\", min_value=128, max_value=256, step=32)\n",
    "    lstm_dropout = hp.Float(\"lstm_dropout\", min_value=0.1, max_value=0.5, step=0.1)\n",
    "    \n",
    "    model.add(Bidirectional(LSTM(lstm_units, return_sequences=True, dropout=lstm_dropout, recurrent_dropout=lstm_dropout)))\n",
    "\n",
    "    # Hiperparámetros para LSTM 2\n",
    "    lstm_units = hp.Int(\"lstm_units_2\", min_value=64, max_value=128, step=32)\n",
    "    lstm_dropout = hp.Float(\"lstm_dropout_2\", min_value=0.1, max_value=0.5, step=0.1)\n",
    "    \n",
    "    model.add(Bidirectional(LSTM(lstm_units, return_sequences=True, dropout=lstm_dropout, recurrent_dropout=lstm_dropout)))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "\n",
    "    # Hiperparámetros para capa densa 1\n",
    "    dense_units = hp.Int(\"dense_units_1\", min_value=32, max_value=128, step=32)\n",
    "    dense_dropout = hp.Float(\"dense_dropout_1\", min_value=0.1, max_value=0.5, step=0.1)\n",
    "    \n",
    "    model.add(Dense(dense_units, activation='relu'))\n",
    "    model.add(Dropout(dense_dropout))\n",
    "\n",
    "    # Hiperparámetros para capa densa 2\n",
    "    dense_units = hp.Int(\"dense_units_2\", min_value=32, max_value=128, step=32)\n",
    "    model.add(Dense(dense_units, activation='relu'))\n",
    "\n",
    "    # Salida del modelo\n",
    "    model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "    # Hiperparámetros para el optimizador (En otras pruebas se vio que Adam era el mejor)\n",
    "    learning_rate = hp.Float(\"learning_rate\", min_value=1e-5, max_value=1e-3, sampling=\"LOG\")\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Define el objeto de búsqueda aleatoria\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective=\"val_accuracy\",\n",
    "    max_trials=20,  # Número de modelos a probar\n",
    "    executions_per_trial=1,\n",
    "    directory='./saved/fine_tuned/',\n",
    "    project_name='HP_LSTM_Glove_text'\n",
    ")\n",
    "\n",
    "# Resumen de la búsqueda\n",
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import Model\n",
    "\n",
    "# def build_gpt_model(hp):\n",
    "#     input_layer = layers.Input(shape=(MAX_LEN, 50))\n",
    "#     x = layers.Bidirectional(layers.LSTM(units=hp.Int('units_1', min_value=64, max_value=256, step=32),\n",
    "#                                          return_sequences=True, dropout=0.25, recurrent_dropout=0.25))(input_layer)\n",
    "#     x = layers.Bidirectional(layers.LSTM(units=hp.Int('units_2', min_value=64, max_value=128, step=32),\n",
    "#                                          return_sequences=True, dropout=0.25, recurrent_dropout=0.25))(x)\n",
    "\n",
    "#     # Aplica la capa de atención usando la salida de la última capa LSTM como query y key\n",
    "#     attention = layers.Attention()([x, x])\n",
    "#     x = layers.GlobalMaxPooling1D()(attention)\n",
    "#     x = layers.Dense(units=hp.Int('dense_units_1', min_value=32, max_value=128, step=32), activation='relu')(x)\n",
    "#     x = layers.Dropout(rate=hp.Float('dropout_1', min_value=0.3, max_value=0.7, step=0.1))(x)\n",
    "#     x = layers.Dense(units=hp.Int('dense_units_2', min_value=32, max_value=128, step=32), activation='relu')(x)\n",
    "#     x = layers.Dropout(rate=hp.Float('dropout_2', min_value=0.3, max_value=0.7, step=0.1))(x)\n",
    "#     output_layer = layers.Dense(7, activation='softmax')(x)\n",
    "\n",
    "#     model = Model(inputs=input_layer, outputs=output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, min_lr=1e-5)\n",
    "cp = ModelCheckpoint('saved/', save_best_only=True)\n",
    "\n",
    "callbacks = [cp, early_stopping, reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=1024\n",
    "tuner.search(X_train, y_train,\n",
    "                    epochs=10,\n",
    "                    validation_split=0.1,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    callbacks=callbacks)\n",
    "\n",
    "best_hp_random = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(\"Mejores hiperparámetros encontrados:\")\n",
    "print(best_hp_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_fine(hp):\n",
    "    model = Sequential()\n",
    "    model.add(layers.Input(shape=(MAX_LEN, 50)))\n",
    "    model.add(layers.Bidirectional(layers.LSTM(units=hp.Int('units', min_value=128, max_value=192, step=16), \n",
    "                                               return_sequences=True, dropout=0.25, recurrent_dropout=0.25)))\n",
    "    model.add(layers.GlobalMaxPooling1D())\n",
    "    model.add(layers.Dense(units=hp.Int('dense_units', min_value=64, max_value=96, step=16), activation='relu'))\n",
    "    model.add(layers.Dropout(rate=hp.Float('dropout', min_value=0.4, max_value=0.6, step=0.05)))\n",
    "    model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "    # Agrega la elección de optimizador como hiperparámetro\n",
    "    optimizer_choice = hp.Choice('optimizer', values=['adam', 'rmsprop'])\n",
    "\n",
    "    # Agrega la elección de learning rate como hiperparámetro\n",
    "    learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-3, sampling='LOG')\n",
    "\n",
    "    if optimizer_choice == 'adam':\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "    else:\n",
    "        optimizer = RMSprop(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "best_model_fine = build_model_fine(best_hp_coarse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=2048\n",
    "history = best_model_fine.fit(X_train, y_train,\n",
    "                    validation_split=0.1, epochs=30,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = best_model_fine.evaluate(X_test, y_test, verbose=1)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T01:14:45.215606Z",
     "start_time": "2023-05-01T01:14:43.478672Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Flatten, Dense, Input, Dropout, Bidirectional, GlobalMaxPooling1D\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(MAX_LEN, 50)))\n",
    "model.add(Bidirectional(LSTM(128, return_sequences=True, dropout=0.25, recurrent_dropout=0.25)))\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True, dropout=0.25, recurrent_dropout=0.25)))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T01:14:56.564506Z",
     "start_time": "2023-05-01T01:14:56.527459Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimizer = Adam(learning_rate=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T01:23:33.795986Z",
     "start_time": "2023-05-01T01:15:01.270492Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE=2048\n",
    "history = model.fit(X_train, y_train,\n",
    "                    validation_split=0.1, epochs=30,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T01:26:05.211462Z",
     "start_time": "2023-05-01T01:25:53.128122Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "lstm_basic_model = load_model('saved/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T01:26:11.168631Z",
     "start_time": "2023-05-01T01:26:05.216463Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score = lstm_basic_model.evaluate(X_test, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T01:26:11.257149Z",
     "start_time": "2023-05-01T01:26:11.092632Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Test Accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Variacion modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-30T19:38:52.074201Z",
     "start_time": "2023-04-30T19:38:50.734201Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lstm_dropout_dense = Sequential(name='Lstm-dout-dense')\n",
    "lstm_dropout_dense.add(Input(shape=(MAX_LEN, 50)))\n",
    "lstm_dropout_dense.add(LSTM(64, return_sequences=True))\n",
    "lstm_dropout_dense.add(Dropout(0.2))\n",
    "lstm_dropout_dense.add(LSTM(32))\n",
    "lstm_dropout_dense.add(Flatten())\n",
    "lstm_dropout_dense.add(Dense(128, activation='relu'))\n",
    "lstm_dropout_dense.add(Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-30T19:38:53.773316Z",
     "start_time": "2023-04-30T19:38:53.717317Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lstm_dropout_dense.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-30T19:49:18.525348Z",
     "start_time": "2023-04-30T19:49:18.260182Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lstm_dropout_dense.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-30T19:49:02.031330Z",
     "start_time": "2023-04-30T19:49:01.983319Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-30T20:33:54.839842Z",
     "start_time": "2023-04-30T19:49:23.531267Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "history_2 = lstm_dropout_dense.fit(X_train, y_train,\n",
    "                    validation_split=0.2, epochs=10,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-30T20:35:26.123724Z",
     "start_time": "2023-04-30T20:34:57.096320Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score = lstm_dropout_dense.evaluate(X_test, y_test, verbose=1)\n",
    "print('Test Accuracy:', score[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
