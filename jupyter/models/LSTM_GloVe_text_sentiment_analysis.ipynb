{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# LSTM-GloVe Model for Sentiment Analysis in text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "En este notebook se describe el proceso de carga, preprocesamiento, embedding, construcción y entrenamiento de un modelo que emplea LSTM y GloVe\n",
    "para el set de datos de texto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Carga de Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "A continuación se cargará el dataset unificado que se construyó en etapas anteriores (Ver `data_join.ipynb` y `NLP_tasks.ipynb`) que cuenta con cas 66000 registros de texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T00:58:40.087569Z",
     "start_time": "2023-05-01T00:58:37.798720Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>@dannycastillo We want to trade with someone w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      0  @tiffanylue i know  i was listenin to bad habi...\n",
       "1      0  Layin n bed with a headache  ughhhh...waitin o...\n",
       "2      0                Funeral ceremony...gloomy friday...\n",
       "3      1               wants to hang out with friends SOON!\n",
       "4      6  @dannycastillo We want to trade with someone w..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('../../data/cleaned/out.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T00:58:53.413010Z",
     "start_time": "2023-05-01T00:58:53.381796Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65989, 2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Preprocesamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Emociones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Se realizará el método de one_hot_encoding para nuestra varible de salida del modelo: Las 7 emociones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T00:58:57.695393Z",
     "start_time": "2023-05-01T00:58:57.633498Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 6 4 5 2 3]\n"
     ]
    }
   ],
   "source": [
    "#Emociones\n",
    "emotions = data['label'].unique()\n",
    "print(emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T00:58:59.181639Z",
     "start_time": "2023-05-01T00:58:59.063532Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65984</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65985</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65986</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65987</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65988</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65989 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0  1  2  3  4  5  6\n",
       "0      1  0  0  0  0  0  0\n",
       "1      1  0  0  0  0  0  0\n",
       "2      1  0  0  0  0  0  0\n",
       "3      0  1  0  0  0  0  0\n",
       "4      0  0  0  0  0  0  1\n",
       "...   .. .. .. .. .. .. ..\n",
       "65984  0  0  0  0  0  0  1\n",
       "65985  0  0  0  0  0  0  1\n",
       "65986  0  0  0  0  0  1  0\n",
       "65987  0  0  0  0  0  0  1\n",
       "65988  0  1  0  0  0  0  0\n",
       "\n",
       "[65989 rows x 7 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = pd.get_dummies(data.label)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Es necesario remover de nuestros datos información irrelevante como etiquetas, puntución, números y caracteres especiales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T00:59:03.264094Z",
     "start_time": "2023-05-01T00:59:03.247716Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@tiffanylue i know  i was listenin to bad habit earlier and i started freakin at his part =['"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "data['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T00:59:06.689496Z",
     "start_time": "2023-05-01T00:59:06.645635Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TAG_RE = re.compile(r'@[^> ]+')\n",
    "\n",
    "def remove_at_sign(sentence: str):\n",
    "    '''\n",
    "    Replaces '@' from and input string for an empty space\n",
    "    :param sentence: String that contains @\n",
    "    :return: sentence without @\n",
    "    '''\n",
    "\n",
    "    return TAG_RE.sub('', sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T00:59:08.978903Z",
     "start_time": "2023-05-01T00:59:08.954464Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' i know  i was listenin to bad habit earlier and i started freakin at his part =['"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_at_sign(data['text'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T00:59:19.178655Z",
     "start_time": "2023-05-01T00:59:11.277560Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/yonosoysantiago/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T00:59:19.191669Z",
     "start_time": "2023-05-01T00:59:19.178655Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def preprocess_text(sentence: str):\n",
    "    '''\n",
    "    Cleans up a sentence leaving only 2 or more non-stopsentences composed of upper and lowercase\n",
    "    :param sentence: String to be cleaned\n",
    "    :return: sentence without numbers, special chars and long stopsentences\n",
    "    '''\n",
    "\n",
    "    cleaned_sentence = sentence.lower()\n",
    "    cleaned_sentence = remove_at_sign(cleaned_sentence)\n",
    "    cleaned_sentence = re.sub('[^a-zA-Z]', ' ', cleaned_sentence)\n",
    "    cleaned_sentence = re.sub('\\s+[a-zA-Z]\\s', ' ', cleaned_sentence)\n",
    "    cleaned_sentence = re.sub('\\s+', ' ', cleaned_sentence)\n",
    "\n",
    "    #Removal of stopsentences\n",
    "    pattern = re.compile(r'\\b(' + r'|'.join(stopwords.words('english')) + r')\\b\\s')\n",
    "    cleaned_sentence = pattern.sub('', cleaned_sentence)\n",
    "\n",
    "    return cleaned_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T00:59:19.300991Z",
     "start_time": "2023-05-01T00:59:19.196670Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' know listenin bad habit earlier started freakin part '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_text(data['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T00:59:55.875793Z",
     "start_time": "2023-05-01T00:59:19.229466Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         know listenin bad habit earlier started freak...\n",
       "1                   layin bed headache ughhhh waitin call \n",
       "2                          funeral ceremony gloomy friday \n",
       "3                                 wants hang friends soon \n",
       "4                  want trade someone houston tickets one \n",
       "                               ...                        \n",
       "65984                                                     \n",
       "65985                       got uh joey women adam apples \n",
       "65986                                  guys messing right \n",
       "65987                                                yeah \n",
       "65988                           good one second like whoa \n",
       "Name: text, Length: 65989, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from copy import  deepcopy\n",
    "\n",
    "cleaned_data = deepcopy(data)\n",
    "cleaned_data['text'] = cleaned_data['text'].apply(preprocess_text)\n",
    "cleaned_data['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Para el proceso de embedding se usarán los datos de un modelo de embedding como lo es GloVe.\n",
    "La información dicho modelo será cargada dentro de `words`. Cada uno de los tokens de GloVe que\n",
    "se usará tiene una dimensión de 50. En caso de que"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T01:00:05.684493Z",
     "start_time": "2023-05-01T00:59:55.881789Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "words = {}\n",
    "\n",
    "def add_to_dict(dictionary, filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            line = line.split(' ')\n",
    "\n",
    "            try:\n",
    "                dictionary[line[0]] = np.array(line[1:], dtype=float)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "add_to_dict(words, './GloVe/glove.6B/glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T01:00:07.113715Z",
     "start_time": "2023-05-01T01:00:05.690516Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Tokenización y Lematización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Una vez cargada la información de los tokens de GloVe se procede a tokenizar y lematizar cada\n",
    "una de las oraciones en nuestro set de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T01:00:07.296718Z",
     "start_time": "2023-05-01T01:00:07.075667Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/yonosoysantiago/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /home/yonosoysantiago/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Ejemplo de cómo se debería de tokenizar y lematizar una oración:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T01:00:09.441765Z",
     "start_time": "2023-05-01T01:00:07.267716Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['know', 'listenin', 'bad', 'habit', 'earlier', 'started', 'freakin', 'part']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "sample = preprocess_text(cleaned_data['text'][0])\n",
    "token_sample = tokenizer.tokenize(sample)\n",
    "lemma_sample = [lemmatizer.lemmatize(token) for token in token_sample]\n",
    "lemma_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "A continuación se define una función para la tokenización y lematización. Adicionalmente, el token final que se entrega únicamente contiene palabras definidas en `words`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T01:00:09.516767Z",
     "start_time": "2023-05-01T01:00:09.446766Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sentence_to_token_list(sentence: str):\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    useful_tokens = [token for token in lemmatized_tokens if token in words]\n",
    "\n",
    "    return  useful_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T01:00:09.531766Z",
     "start_time": "2023-05-01T01:00:09.461765Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['know', 'bad', 'habit', 'earlier', 'started', 'freakin', 'part']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_to_token_list(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Con el token anterior, el cual sabemos se puede representar por medio de uno de los tokens almacenados en `words`, entonces pasamos a la representación de estos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T01:00:09.532765Z",
     "start_time": "2023-05-01T01:00:09.477767Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sentence_to_words_vectors(sentence: str, word_dict=words):\n",
    "    processed_tokens = sentence_to_token_list(sentence)\n",
    "\n",
    "    vectors = []\n",
    "    for token in processed_tokens:\n",
    "        if token in word_dict:\n",
    "            token_vector = word_dict[token]\n",
    "            vectors.append(token_vector)\n",
    "\n",
    "    return np.array(vectors, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T01:00:09.532765Z",
     "start_time": "2023-05-01T01:00:09.494768Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 50)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_to_words_vectors(sample).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T01:00:09.545765Z",
     "start_time": "2023-05-01T01:00:09.515769Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.28328  ,  0.12887  ,  0.29861  , -0.23238  ,  0.88263  ,\n",
       "        -0.30814  , -0.61148  ,  0.37254  , -0.40449  ,  0.17351  ,\n",
       "        -0.13958  ,  0.67868  , -0.55673  , -0.13453  ,  1.1441   ,\n",
       "         0.53457  ,  0.59148  ,  0.19083  ,  0.25406  , -0.71581  ,\n",
       "        -0.87951  ,  0.72879  ,  0.67054  ,  0.62866  ,  1.0034   ,\n",
       "        -2.2327   , -1.194    , -0.06245  ,  0.93933  , -1.1132   ,\n",
       "         2.926    ,  0.59809  , -0.51255  , -0.49886  , -0.016524 ,\n",
       "        -0.20697  , -0.059229 , -0.0072689,  0.53314  , -0.14621  ,\n",
       "        -0.21417  ,  0.24102  ,  0.21648  ,  0.73351  ,  0.43582  ,\n",
       "         0.045278 , -0.25489  ,  0.15273  , -0.13088  ,  0.89397  ],\n",
       "       [-0.17981  , -0.40407  , -0.1653   , -0.60687  , -0.39656  ,\n",
       "         0.12688  , -0.053049 ,  0.38024  , -0.51008  ,  0.46593  ,\n",
       "        -0.30818  ,  0.79362  , -0.85766  , -0.25143  ,  1.0448   ,\n",
       "         0.18628  ,  0.13688  ,  0.092588 , -0.2236   , -0.13604  ,\n",
       "        -0.19482  ,  0.057702 ,  0.56133  ,  0.24823  ,  0.627    ,\n",
       "        -1.8437   , -1.2573   ,  0.64482  ,  1.2787   , -0.29522  ,\n",
       "         3.0493   ,  0.62079  ,  0.90369  , -0.030099 , -0.13091  ,\n",
       "         0.30525  , -0.070138 , -0.12912  ,  0.72277  , -0.79774  ,\n",
       "        -0.70277  ,  0.038009 ,  0.27192  ,  0.35679  ,  0.26493  ,\n",
       "         0.13037  , -0.01369  ,  0.33713  ,  0.99956  ,  0.72031  ],\n",
       "       [-0.031058 , -0.24609  , -0.9077   , -1.0229   ,  0.19356  ,\n",
       "         0.53121  , -0.21515  , -0.40683  ,  0.59225  ,  1.0359   ,\n",
       "        -0.18254  ,  0.36191  ,  0.18725  , -0.0078135,  0.62996  ,\n",
       "        -0.46528  , -0.44288  ,  0.12539  , -0.0065608, -0.4708   ,\n",
       "        -0.42531  , -0.26501  ,  0.38555  ,  0.34596  ,  0.35707  ,\n",
       "        -1.2948   ,  0.42021  ,  0.45468  ,  0.91855  ,  0.14189  ,\n",
       "         1.582    ,  0.5803   ,  0.088518 , -0.033661 , -0.66021  ,\n",
       "         0.23344  , -0.70859  ,  0.52198  , -0.21834  , -0.24428  ,\n",
       "        -0.21386  , -0.27636  , -0.61687  ,  1.6641   ,  1.1188   ,\n",
       "        -0.23147  ,  0.19004  , -0.60125  ,  0.11713  , -0.25198  ],\n",
       "       [ 0.53772  , -0.17572  ,  0.16704  ,  0.2617   ,  0.0095107,\n",
       "         0.17802  , -0.8192   ,  0.023313 , -0.67016  , -0.18026  ,\n",
       "        -0.15323  , -0.70254  , -0.66864  , -0.28519  ,  0.99582  ,\n",
       "         0.11573  , -0.94466  , -0.87298  , -0.77195  ,  0.081174 ,\n",
       "         0.46313  ,  0.30321  ,  0.34776  , -0.97389  , -0.011876 ,\n",
       "        -1.5462   ,  0.23966  ,  0.18437  , -0.65125  ,  0.46257  ,\n",
       "         3.2985   , -0.31677  , -0.067658 ,  0.0084145,  0.22632  ,\n",
       "        -0.55538  ,  0.49637  ,  0.098167 , -0.32241  , -0.057772 ,\n",
       "        -0.39908  ,  0.42339  ,  0.07051  , -0.49125  ,  0.20524  ,\n",
       "        -0.068185 , -0.65967  ,  0.71609  , -0.017516 , -0.54233  ],\n",
       "       [-0.20492  , -0.47264  , -0.24182  , -0.42806  , -0.58667  ,\n",
       "        -0.29953  , -1.5043   ,  0.22041  , -0.32192  , -0.011014 ,\n",
       "         0.24347  , -0.13998  , -1.0299   , -0.076541 ,  0.57273  ,\n",
       "        -0.17379  , -0.29957  ,  0.25672  , -0.77973  , -0.053743 ,\n",
       "         0.89019  ,  0.36763  ,  0.40332  ,  0.20212  ,  0.40951  ,\n",
       "        -1.1495   ,  0.18819  , -0.27845  ,  0.22702  , -0.42631  ,\n",
       "         3.175    ,  0.32766  , -0.275    , -0.25166  , -0.21405  ,\n",
       "         0.2567   , -0.09849  ,  0.5659   ,  0.31893  ,  0.26182  ,\n",
       "        -0.44939  , -0.38002  , -0.54841  , -0.29008  ,  0.20738  ,\n",
       "        -0.060745 ,  0.26459  , -0.58691  , -0.31341  , -0.20058  ],\n",
       "       [-0.081298 , -0.18447  , -0.20424  , -1.0914   ,  0.55381  ,\n",
       "        -0.91321  , -0.15558  , -0.13129  , -0.14143  ,  1.0235   ,\n",
       "        -0.33015  ,  0.81114  , -0.17682  ,  0.40954  , -0.16874  ,\n",
       "         0.17236  ,  0.42768  , -0.042834 , -0.066574 ,  0.29009  ,\n",
       "        -1.0208   ,  0.073951 , -0.053494 ,  0.067791 ,  0.24571  ,\n",
       "         0.29656  , -1.3656   ,  0.43038  ,  0.8005   , -0.41209  ,\n",
       "        -0.51373  ,  1.1757   ,  0.077595 ,  1.1339   ,  0.35349  ,\n",
       "        -0.14229  ,  0.85759  , -0.905    ,  0.18951  ,  0.19021  ,\n",
       "        -0.56274  , -0.063354 , -1.0776   ,  0.6256   , -0.075322 ,\n",
       "         0.12923  , -0.34188  , -0.027863 ,  0.64354  ,  0.59983  ],\n",
       "       [ 0.70504  ,  0.18255  , -0.75188  ,  0.0039397, -0.053423 ,\n",
       "         0.29625  , -0.42377  , -0.013111 ,  0.16573  , -0.43932  ,\n",
       "         0.07506  , -0.16262  , -0.26635  , -0.40453  , -0.098246 ,\n",
       "         0.14215  ,  0.34546  ,  0.3634   , -0.40502  ,  0.3591   ,\n",
       "         0.37462  , -0.33984  , -0.24689  ,  0.33546  , -0.36193  ,\n",
       "        -1.7587   , -0.72343  ,  0.14269  , -0.099759 , -0.16071  ,\n",
       "         3.6203   , -0.25283  , -0.24719  , -0.53168  , -0.050993 ,\n",
       "         0.017225 , -0.12073  , -0.021644 , -0.36179  , -0.086212 ,\n",
       "        -0.32108  , -0.16623  , -0.2044   , -0.46124  , -0.85036  ,\n",
       "         0.16189  , -0.14281  , -0.13137  , -0.41208  , -0.092147 ]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_to_words_vectors(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T01:07:08.842774Z",
     "start_time": "2023-05-01T01:06:58.574974Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [[0.28328, 0.12887, 0.29861, -0.23238, 0.88263...\n",
       "1        [[0.062774, 0.25423, 0.54447, -0.49013, -0.488...\n",
       "2        [[0.89902, 1.5889, -0.34232, -0.40286, 0.97148...\n",
       "3        [[0.13627, -0.054478, 0.3703, -0.41574, 0.6056...\n",
       "4        [[0.13627, -0.054478, 0.3703, -0.41574, 0.6056...\n",
       "                               ...                        \n",
       "65984                                                   []\n",
       "65985    [[-0.4097, -0.37167, 0.38852, -0.34947, 0.5425...\n",
       "65986    [[-0.76156, -0.012958, -0.14249, -0.5142, 1.50...\n",
       "65987    [[-0.80924, -0.030977, 0.5102, -0.75298, 0.490...\n",
       "65988    [[-0.35586, 0.5213, -0.6107, -0.30131, 0.94862...\n",
       "Name: text, Length: 65989, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Se obtiene nuestro conjunto de datos X\n",
    "X = cleaned_data['text'].apply(lambda sentence: sentence_to_words_vectors(sentence))\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Dado que las matrices de vectores de cada oración tienen un número diferente de filas debido a que cada oración cuenta con un número diferente de palabas. Es necesario identificar el tamaño máximo de los textos que se tienen para su \"estandarización\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T01:08:30.301341Z",
     "start_time": "2023-05-01T01:08:30.182293Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temporal = deepcopy(X)\n",
    "temporal['len'] = temporal.apply(np.shape)\n",
    "\n",
    "MAX_LEN = max(temporal['len'])[0]\n",
    "MAX_LEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Dado que el tamaño máximo es 35, entonces se llevarán todas las matrices a la forma `(35, 50)`. Los valores faltantes para cada vector serán 0s en su inicio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T01:11:58.362852Z",
     "start_time": "2023-05-01T01:11:53.413029Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-01 17:23:33.737629: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-01 17:23:34.351244: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/yonosoysantiago/miniconda3/envs/tf211_conda/lib/\n",
      "2023-05-01 17:23:34.351308: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/yonosoysantiago/miniconda3/envs/tf211_conda/lib/\n",
      "2023-05-01 17:23:34.351313: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "X_ = tf.keras.utils.pad_sequences(X, maxlen=MAX_LEN, dtype='float32')\n",
    "# X_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T01:12:03.087480Z",
     "start_time": "2023-05-01T01:12:03.061449Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65989, 35, 50)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T01:12:06.580315Z",
     "start_time": "2023-05-01T01:12:06.539900Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35, 50)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T01:12:09.081556Z",
     "start_time": "2023-05-01T01:12:09.056503Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
       "         0.      ],\n",
       "       [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
       "         0.      ],\n",
       "       [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
       "         0.      ],\n",
       "       ...,\n",
       "       [-0.20492 , -0.47264 , -0.24182 , ..., -0.58691 , -0.31341 ,\n",
       "        -0.20058 ],\n",
       "       [-0.081298, -0.18447 , -0.20424 , ..., -0.027863,  0.64354 ,\n",
       "         0.59983 ],\n",
       "       [ 0.70504 ,  0.18255 , -0.75188 , ..., -0.13137 , -0.41208 ,\n",
       "        -0.092147]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## División Entrenamiento-Validación-Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T01:12:40.748468Z",
     "start_time": "2023-05-01T01:12:39.779816Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test =  train_test_split(X_, y, test_size=0.2, random_state=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Modelos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_80214/3764173312.py:5: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
      "  from kerastuner import RandomSearch, HyperParameters\n",
      "2023-05-01 17:23:35.543846: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-01 17:23:35.544039: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-01 17:23:35.548317: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-01 17:23:35.548500: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-01 17:23:35.548647: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-01 17:23:35.548787: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-01 17:23:35.549141: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-01 17:23:35.670790: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-01 17:23:35.670986: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-01 17:23:35.671131:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-01 17:23:35.671265: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-01 17:23:35.671396: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-01 17:23:35.671526: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-01 17:23:36.451870: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-01 17:23:36.452102: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-01 17:23:36.452279: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-01 17:23:36.452421: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-01 17:23:36.452568: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-01 17:23:36.452683: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5388 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "2023-05-01 17:23:36.452907: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-01 17:23:36.453018: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 2616 MB memory:  -> device: 1, name: NVIDIA GeForce GTX 1650, pci bus id: 0000:06:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Search space summary\n",
      "Default search space size: 8\n",
      "lstm_units_1 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 128, 'max_value': 256, 'step': 32, 'sampling': 'linear'}\n",
      "lstm_dropout (Float)\n",
      "{'default': 0.1, 'conditions': [], 'min_value': 0.1, 'max_value': 0.5, 'step': 0.1, 'sampling': 'linear'}\n",
      "lstm_units_2 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 64, 'max_value': 128, 'step': 32, 'sampling': 'linear'}\n",
      "lstm_dropout_2 (Float)\n",
      "{'default': 0.1, 'conditions': [], 'min_value': 0.1, 'max_value': 0.5, 'step': 0.1, 'sampling': 'linear'}\n",
      "dense_units_1 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 128, 'step': 32, 'sampling': 'linear'}\n",
      "dense_dropout_1 (Float)\n",
      "{'default': 0.1, 'conditions': [], 'min_value': 0.1, 'max_value': 0.5, 'step': 0.1, 'sampling': 'linear'}\n",
      "dense_units_2 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 128, 'step': 32, 'sampling': 'linear'}\n",
      "learning_rate (Float)\n",
      "{'default': 1e-05, 'conditions': [], 'min_value': 1e-05, 'max_value': 0.001, 'step': None, 'sampling': 'log'}\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.layers import Embedding, Lambda, LSTM, Flatten, Dense, Input, Dropout, Bidirectional, GlobalMaxPooling1D\n",
    "from keras.optimizers import Adam, RMSprop, SGD\n",
    "from kerastuner import RandomSearch, HyperParameters\n",
    "\n",
    "def build_model(hp):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(MAX_LEN, 50)))\n",
    "\n",
    "    # Hiperparámetros para LSTM 1\n",
    "    lstm_units = hp.Int(\"lstm_units_1\", min_value=128, max_value=256, step=32)\n",
    "    lstm_dropout = hp.Float(\"lstm_dropout\", min_value=0.1, max_value=0.5, step=0.1)\n",
    "    \n",
    "    model.add(Bidirectional(LSTM(lstm_units, return_sequences=True, dropout=lstm_dropout, recurrent_dropout=lstm_dropout)))\n",
    "\n",
    "    # Hiperparámetros para LSTM 2\n",
    "    lstm_units = hp.Int(\"lstm_units_2\", min_value=64, max_value=128, step=32)\n",
    "    lstm_dropout = hp.Float(\"lstm_dropout_2\", min_value=0.1, max_value=0.5, step=0.1)\n",
    "    \n",
    "    model.add(Bidirectional(LSTM(lstm_units, return_sequences=True, dropout=lstm_dropout, recurrent_dropout=lstm_dropout)))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "\n",
    "    # Hiperparámetros para capa densa 1\n",
    "    dense_units = hp.Int(\"dense_units_1\", min_value=32, max_value=128, step=32)\n",
    "    dense_dropout = hp.Float(\"dense_dropout_1\", min_value=0.1, max_value=0.5, step=0.1)\n",
    "    \n",
    "    model.add(Dense(dense_units, activation='relu'))\n",
    "    model.add(Dropout(dense_dropout))\n",
    "\n",
    "    # Hiperparámetros para capa densa 2\n",
    "    dense_units = hp.Int(\"dense_units_2\", min_value=32, max_value=128, step=32)\n",
    "    model.add(Dense(dense_units, activation='relu'))\n",
    "\n",
    "    # Salida del modelo\n",
    "    model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "    # Hiperparámetros para el optimizador (En otras pruebas se vio que Adam era el mejor)\n",
    "    learning_rate = hp.Float(\"learning_rate\", min_value=1e-5, max_value=1e-3, sampling=\"LOG\")\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Define el objeto de búsqueda aleatoria\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective=\"val_accuracy\",\n",
    "    max_trials=20,  # Número de modelos a probar\n",
    "    executions_per_trial=1,\n",
    "    directory='./saved/fine_tuned/',\n",
    "    project_name='HP_LSTM_Glove_text'\n",
    ")\n",
    "\n",
    "# Resumen de la búsqueda\n",
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, min_lr=1e-5)\n",
    "cp = ModelCheckpoint('saved/', save_best_only=True)\n",
    "\n",
    "callbacks = [cp, early_stopping, reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 11 Complete [00h 03m 59s]\n",
      "val_accuracy: 0.35075756907463074\n",
      "\n",
      "Best val_accuracy So Far: 0.45113635063171387\n",
      "Total elapsed time: 00h 44m 16s\n",
      "\n",
      "Search: Running Trial #12\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "160               |224               |lstm_units_1\n",
      "0.5               |0.3               |lstm_dropout\n",
      "64                |96                |lstm_units_2\n",
      "0.3               |0.4               |lstm_dropout_2\n",
      "96                |128               |dense_units_1\n",
      "0.3               |0.3               |dense_dropout_1\n",
      "32                |96                |dense_units_2\n",
      "0.00082346        |0.00058203        |learning_rate\n",
      "\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/10\n",
      "47/47 [==============================] - ETA: 0s - loss: 1.8245 - accuracy: 0.2613INFO:tensorflow:Assets written to: saved/assets\n",
      "47/47 [==============================] - 30s 496ms/step - loss: 1.8245 - accuracy: 0.2613 - val_loss: 1.7318 - val_accuracy: 0.3345 - lr: 8.2346e-04\n",
      "Epoch 2/10\n",
      "47/47 [==============================] - ETA: 0s - loss: 1.7028 - accuracy: 0.3444INFO:tensorflow:Assets written to: saved/assets\n",
      "47/47 [==============================] - 23s 493ms/step - loss: 1.7028 - accuracy: 0.3444 - val_loss: 1.6367 - val_accuracy: 0.3775 - lr: 8.2346e-04\n",
      "Epoch 3/10\n",
      "47/47 [==============================] - ETA: 0s - loss: 1.6496 - accuracy: 0.3677INFO:tensorflow:Assets written to: saved/assets\n",
      "47/47 [==============================] - 23s 502ms/step - loss: 1.6496 - accuracy: 0.3677 - val_loss: 1.5990 - val_accuracy: 0.3938 - lr: 8.2346e-04\n",
      "Epoch 4/10\n",
      "47/47 [==============================] - ETA: 0s - loss: 1.6247 - accuracy: 0.3796INFO:tensorflow:Assets written to: saved/assets\n",
      "47/47 [==============================] - 24s 517ms/step - loss: 1.6247 - accuracy: 0.3796 - val_loss: 1.5723 - val_accuracy: 0.4015 - lr: 8.2346e-04\n",
      "Epoch 5/10\n",
      "47/47 [==============================] - ETA: 0s - loss: 1.6033 - accuracy: 0.3871INFO:tensorflow:Assets written to: saved/assets\n",
      "47/47 [==============================] - 24s 518ms/step - loss: 1.6033 - accuracy: 0.3871 - val_loss: 1.5570 - val_accuracy: 0.4038 - lr: 8.2346e-04\n",
      "Epoch 6/10\n",
      "47/47 [==============================] - ETA: 0s - loss: 1.5846 - accuracy: 0.3932INFO:tensorflow:Assets written to: saved/assets\n",
      "47/47 [==============================] - 23s 501ms/step - loss: 1.5846 - accuracy: 0.3932 - val_loss: 1.5413 - val_accuracy: 0.4108 - lr: 8.2346e-04\n",
      "Epoch 7/10\n",
      "47/47 [==============================] - ETA: 0s - loss: 1.5692 - accuracy: 0.4025INFO:tensorflow:Assets written to: saved/assets\n",
      "47/47 [==============================] - 23s 492ms/step - loss: 1.5692 - accuracy: 0.4025 - val_loss: 1.5226 - val_accuracy: 0.4212 - lr: 8.2346e-04\n",
      "Epoch 8/10\n",
      "47/47 [==============================] - ETA: 0s - loss: 1.5541 - accuracy: 0.4081INFO:tensorflow:Assets written to: saved/assets\n",
      "47/47 [==============================] - 24s 514ms/step - loss: 1.5541 - accuracy: 0.4081 - val_loss: 1.5087 - val_accuracy: 0.4250 - lr: 8.2346e-04\n",
      "Epoch 9/10\n",
      "47/47 [==============================] - ETA: 0s - loss: 1.5404 - accuracy: 0.4150"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE=1024\n",
    "tuner.search(X_train, y_train,\n",
    "                    epochs=10,\n",
    "                    validation_split=0.1,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    callbacks=callbacks)\n",
    "\n",
    "best_hp_random = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(\"Mejores hiperparámetros encontrados:\")\n",
    "print(best_hp_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T01:14:45.215606Z",
     "start_time": "2023-05-01T01:14:43.478672Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from keras.models import Sequential\n",
    "# from keras.layers import Embedding, LSTM, Flatten, Dense, Input, Dropout, Bidirectional, GlobalMaxPooling1D\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Input(shape=(MAX_LEN, 50)))\n",
    "# model.add(Bidirectional(LSTM(128, return_sequences=True, dropout=0.25, recurrent_dropout=0.25)))\n",
    "# model.add(Bidirectional(LSTM(64, return_sequences=True, dropout=0.25, recurrent_dropout=0.25)))\n",
    "# model.add(GlobalMaxPooling1D())\n",
    "# model.add(Dense(128, activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(64, activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T01:26:05.211462Z",
     "start_time": "2023-05-01T01:25:53.128122Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "lstm_basic_model = load_model('saved/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T01:26:11.168631Z",
     "start_time": "2023-05-01T01:26:05.216463Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score = lstm_basic_model.evaluate(X_test, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T01:26:11.257149Z",
     "start_time": "2023-05-01T01:26:11.092632Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Test Accuracy:', score[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## GloVe-LSTM-DO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-30T19:38:52.074201Z",
     "start_time": "2023-04-30T19:38:50.734201Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lstm_dropout_dense = Sequential(name='Lstm-dout-dense')\n",
    "lstm_dropout_dense.add(Input(shape=(MAX_LEN, 50)))\n",
    "lstm_dropout_dense.add(LSTM(64, return_sequences=True))\n",
    "lstm_dropout_dense.add(Dropout(0.2))\n",
    "lstm_dropout_dense.add(LSTM(32))\n",
    "lstm_dropout_dense.add(Flatten())\n",
    "lstm_dropout_dense.add(Dense(128, activation='relu'))\n",
    "lstm_dropout_dense.add(Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Input, LSTM, Dropout, Flatten, Dense\n",
    "from keras.optimizers import Adam, RMSprop, SGD\n",
    "from kerastuner import RandomSearch, HyperParameters\n",
    "\n",
    "def build_model_gloveLstmDo(hp):\n",
    "    model = Sequential(name='GloVe-LSTM-DO')\n",
    "    model.add(Input(shape=(MAX_LEN, 50)))\n",
    "    \n",
    "    # Hiperparámetros para la primera capa LSTM\n",
    "    lstm_units_1 = hp.Int(\"lstm_units_1\", min_value=32, max_value=128, step=32)\n",
    "    model.add(LSTM(lstm_units_1, return_sequences=True))\n",
    "    \n",
    "    # Hiperparámetros para Dropout\n",
    "    dropout_rate = hp.Float(\"dropout_rate\", min_value=0.1, max_value=0.5, step=0.1)\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    # Hiperparámetros para la segunda capa LSTM\n",
    "    lstm_units_2 = hp.Int(\"lstm_units_2\", min_value=16, max_value=64, step=16)\n",
    "    model.add(LSTM(lstm_units_2))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    # Hiperparámetros para la capa Densa\n",
    "    dense_units = hp.Int(\"dense_units\", min_value=32, max_value=256, step=32)\n",
    "    model.add(Dense(dense_units, activation='relu'))\n",
    "    \n",
    "    model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "    # Hiperparámetros para el optimizador\n",
    "    learning_rate = hp.Float(\"learning_rate\", min_value=1e-5, max_value=1e-3, sampling=\"LOG\")\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "gloveLstmDoTuner = RandomSearch(\n",
    "    build_model_gloveLstmDo,\n",
    "    objective=\"val_accuracy\",\n",
    "    max_trials=15,  # Número de modelos a probar\n",
    "    executions_per_trial=1,\n",
    "    directory='./saved/fine_tuned/',\n",
    "    project_name='HP_LSTM_dropout_dense'\n",
    ")\n",
    "\n",
    "# Resumen de la búsqueda\n",
    "gloveLstmDoTuner.search_space_summary()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realiza la búsqueda\n",
    "gloveLstmDoTuner.search(X_train, y_train,\n",
    "             epochs=10,\n",
    "             validation_split=0.1,\n",
    "             batch_size=4096,\n",
    "             callbacks=callbacks)\n",
    "\n",
    "best_hp_random_2 = gloveLstmDoTuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(\"Mejores hiperparámetros encontrados:\")\n",
    "print(best_hp_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-30T19:38:53.773316Z",
     "start_time": "2023-04-30T19:38:53.717317Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lstm_dropout_dense.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-30T19:49:18.525348Z",
     "start_time": "2023-04-30T19:49:18.260182Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lstm_dropout_dense.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-30T19:49:02.031330Z",
     "start_time": "2023-04-30T19:49:01.983319Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-30T20:33:54.839842Z",
     "start_time": "2023-04-30T19:49:23.531267Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "history_2 = lstm_dropout_dense.fit(X_train, y_train,\n",
    "                    validation_split=0.2, epochs=10,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-30T20:35:26.123724Z",
     "start_time": "2023-04-30T20:34:57.096320Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score = lstm_dropout_dense.evaluate(X_test, y_test, verbose=1)\n",
    "print('Test Accuracy:', score[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3LSTM-DO-CNN-Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "\n",
    "lstm3_do_cnn_dense = Sequential(name='3LSTM-DO-CNN-Dense')\n",
    "lstm3_do_cnn_dense.add(Input(shape=(MAX_LEN, 50)))\n",
    "\n",
    "#LSTM\n",
    "lstm3_do_cnn_dense.add(LSTM(256, name='LSTM1', return_sequences=True))\n",
    "lstm3_do_cnn_dense.add(Dropout(0.2, name='DO1'))\n",
    "\n",
    "lstm3_do_cnn_dense.add(LSTM(128, name='LSTM2', return_sequences=True))\n",
    "lstm3_do_cnn_dense.add(Dropout(0.2, name='DO2'))\n",
    "\n",
    "lstm3_do_cnn_dense.add(LSTM(64, name='LSTM3', return_sequences=True))\n",
    "lstm3_do_cnn_dense.add(Dropout(0.2, name='DO3'))\n",
    "\n",
    "#CNN\n",
    "lstm3_do_cnn_dense.add(Conv1D(128, kernel_size=3, strides=1, padding='same', activation='relu'))\n",
    "lstm3_do_cnn_dense.add(MaxPooling1D(pool_size=3, strides=2, padding='same'))\n",
    "lstm3_do_cnn_dense.add(Dropout(0.2))\n",
    "lstm3_do_cnn_dense.add(Flatten(name='F1'))\n",
    "\n",
    "#Fully connected\n",
    "lstm3_do_cnn_dense.add(Dense(64, activation='relu'))\n",
    "lstm3_do_cnn_dense.add(Dense(7, activation='softmax'))\n",
    "\n",
    "lstm3_do_cnn_dense.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_gloveLstmDoCnn(hp):\n",
    "    model = Sequential(name='3LSTM-DO-CNN-Dense')\n",
    "    model.add(Input(shape=(MAX_LEN, 50)))\n",
    "\n",
    "    # Hiperparámetros para la primera capa LSTM\n",
    "    lstm_units_1 = hp.Int(\"lstm_units_1\", min_value=128, max_value=512, step=32)\n",
    "    model.add(LSTM(lstm_units_1, name='LSTM1', return_sequences=True))\n",
    "    lstm_dropout_1 = hp.Float(\"lstm_dropout_1\", min_value=0.1, max_value=0.5, step=0.1)\n",
    "    model.add(Dropout(lstm_dropout_1, name='DO1'))\n",
    "\n",
    "    # Hiperparámetros para la segunda capa LSTM\n",
    "    lstm_units_2 = hp.Int(\"lstm_units_2\", min_value=64, max_value=256, step=32)\n",
    "    model.add(LSTM(lstm_units_2, name='LSTM2', return_sequences=True))\n",
    "    lstm_dropout_2 = hp.Float(\"lstm_dropout_2\", min_value=0.1, max_value=0.5, step=0.1)\n",
    "    model.add(Dropout(lstm_dropout_2, name='DO2'))\n",
    "\n",
    "    # Hiperparámetros para la tercera capa LSTM\n",
    "    lstm_units_3 = hp.Int(\"lstm_units_3\", min_value=32, max_value=128, step=16)\n",
    "    model.add(LSTM(lstm_units_3, name='LSTM3', return_sequences=True))\n",
    "    lstm_dropout_3 = hp.Float(\"lstm_dropout_3\", min_value=0.1, max_value=0.5, step=0.1)\n",
    "    model.add(Dropout(lstm_dropout_3, name='DO3'))\n",
    "\n",
    "    # Hiperparámetros para la capa Conv1D\n",
    "    conv_filters = hp.Int(\"conv_filters\", min_value=32, max_value=256, step=32)\n",
    "    model.add(Conv1D(conv_filters, kernel_size=3, strides=1, padding='same', activation='relu'))\n",
    "    \n",
    "    model.add(MaxPooling1D(pool_size=3, strides=2, padding='same'))\n",
    "\n",
    "    cnn_dropout = hp.Float(\"cnn_dropout\", min_value=0.1, max_value=0.5, step=0.1)\n",
    "    model.add(Dropout(cnn_dropout))\n",
    "    \n",
    "    model.add(Flatten(name='F1'))\n",
    "\n",
    "    # Hiperparámetros para la capa Densa\n",
    "    dense_units = hp.Int(\"dense_units\", min_value=32, max_value=256, step=32)\n",
    "    model.add(Dense(dense_units, activation='relu'))\n",
    "    \n",
    "    model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "    # Hiperparámetros para el optimizador\n",
    "    learning_rate = hp.Float(\"learning_rate\", min_value=1e-5, max_value=1e-3, sampling=\"LOG\")\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective=\"val_accuracy\",\n",
    "    max_trials=15,  # Número de modelos a probar\n",
    "    executions_per_trial=1,\n",
    "    directory='./saved/fine_tuned/',\n",
    "    project_name='HP_3LSTM-DO-CNN-Dense'\n",
    ")\n",
    "\n",
    "# Resumen de la búsqueda\n",
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=2048\n",
    "tuner.search(X_train, y_train,\n",
    "                    epochs=10,\n",
    "                    validation_split=0.1,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    callbacks=callbacks)\n",
    "\n",
    "best_hp_random = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(\"Mejores hiperparámetros encontrados:\")\n",
    "print(best_hp_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm3_do_cnn_dense.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "history_3 = lstm3_do_cnn_dense.fit(X_train, y_train,\n",
    "                    validation_split=0.2, epochs=10,\n",
    "                    callbacks=[early_stop])\n",
    "\n",
    "score = lstm_dropout_dense.evaluate(X_test, y_test, verbose=1)\n",
    "print('Test Accuracy:', score[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
