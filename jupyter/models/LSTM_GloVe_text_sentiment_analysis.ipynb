{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM-GloVe Model for Sentiment Analysis in text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este notebook se describe el proceso de carga, preprocesamiento, embedding, construcción y entrenamiento de un modelo que emplea LSTM y GloVe\n",
    "para el set de datos de texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-16T11:11:01.750681Z",
     "end_time": "2023-05-16T11:11:02.078805Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA_PATH = '../../data/text/cleaned/final.csv'\n",
    "DATA_REDUCED_PATH = '../../data/text/cleaned/reduced_final_eng_clean.csv'\n",
    "DATA_OUTPUT_PATH = '../../data/text/cleaned/preprocessed/final.pkl'\n",
    "CHUNK_SIZE = 10**6\n",
    "COLUMNS = ['text','priority']\n",
    "words = {}\n",
    "MAX_LEN = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRE PROCESAMIENTO DE DATOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocesamiento\n",
    "Es necesario remover de nuestros datos información irrelevante como etiquetas, puntución, números y caracteres especiales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-16T11:11:01.797577Z",
     "end_time": "2023-05-16T11:11:02.110062Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "TAG_RE = re.compile(r'@[^> ]+')\n",
    "\n",
    "def remove_at_sign(sentence: str):\n",
    "    '''\n",
    "    Replaces '@' from and input string for an empty space\n",
    "    :param sentence: String that contains @\n",
    "    :return: sentence without @\n",
    "    '''\n",
    "\n",
    "    return TAG_RE.sub('', sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-16T11:11:01.813183Z",
     "end_time": "2023-05-16T11:11:03.412785Z"
    }
   },
   "outputs": [],
   "source": [
    "import langid\n",
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "def translate_sentence(sentence: str):\n",
    "    \"\"\"\n",
    "    Translate a sentence to english if it's in different language.\n",
    "    :param sentence: The string/sentence to translate\n",
    "    :return: The original sentence in english\n",
    "    \"\"\"\n",
    "    lang = langid.classify(sentence)[0]\n",
    "    if lang != 'en' and len(sentence) < 5000:\n",
    "        sentence = GoogleTranslator(source='auto').translate(sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-16T11:11:03.412785Z",
     "end_time": "2023-05-16T11:11:07.585600Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    },
    {
     "data": {
      "text/plain": "False"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-16T14:26:43.682808Z",
     "end_time": "2023-05-16T14:26:43.714184Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def preprocess_text(sentence: str):\n",
    "    '''\n",
    "    Cleans up a sentence leaving only 2 or more non-stopwords composed of upper and lowercase\n",
    "    :param sentence: String to be cleaned\n",
    "    :return: sentence without numbers, special chars and long stopwords\n",
    "    '''\n",
    "\n",
    "    cleaned_sentence = sentence.lower()\n",
    "    cleaned_sentence = remove_at_sign(cleaned_sentence)\n",
    "    cleaned_sentence = re.sub('[^a-zA-Z]', ' ', cleaned_sentence)\n",
    "    cleaned_sentence = re.sub('\\s+[a-zA-Z]\\s', ' ', cleaned_sentence)\n",
    "    cleaned_sentence = re.sub('\\s+', ' ', cleaned_sentence)\n",
    "\n",
    "    #Translate\n",
    "    #cleaned_sentence = translate_sentence(cleaned_sentence)\n",
    "\n",
    "    #Removal of stopwords\n",
    "    pattern = re.compile(r'\\b(' + r'|'.join(stopwords.words('english')) + r')\\b\\s')\n",
    "    cleaned_sentence = pattern.sub('', cleaned_sentence)\n",
    "\n",
    "    return cleaned_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-16T11:11:07.601185Z",
     "end_time": "2023-05-16T11:11:07.679357Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def add_to_dict(dictionary, filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            line = line.split(' ')\n",
    "\n",
    "            try:\n",
    "                dictionary[line[0]] = np.array(line[1:], dtype=float)\n",
    "            except:\n",
    "                continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenización y Lematización\n",
    "Una vez cargada la información de los tokens de GloVe se procede a tokenizar y lematizar cada\n",
    "una de las oraciones en nuestro set de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-16T11:11:07.616819Z",
     "end_time": "2023-05-16T11:11:07.679357Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading wordnet: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n",
      "[nltk_data] Error loading omw-1.4: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    },
    {
     "data": {
      "text/plain": "False"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-16T11:11:07.663698Z",
     "end_time": "2023-05-16T11:11:07.679357Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def sentence_to_token_list(sentence: str):\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    useful_tokens = [token for token in lemmatized_tokens if token in words]\n",
    "\n",
    "    return  useful_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el token anterior, el cual sabemos se puede representar por medio de uno de los tokens almacenados en `words`, entonces pasamos a la representación de estos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-16T11:11:07.679357Z",
     "end_time": "2023-05-16T11:11:07.757445Z"
    }
   },
   "outputs": [],
   "source": [
    "def sentence_to_words_vectors(sentence: str, word_dict=words):\n",
    "    processed_tokens = sentence_to_token_list(sentence)\n",
    "\n",
    "    vectors = []\n",
    "    for token in processed_tokens:\n",
    "        if token in word_dict:\n",
    "            token_vector = word_dict[token]\n",
    "            vectors.append(token_vector)\n",
    "    \n",
    "\n",
    "    array = np.array(vectors, dtype=float)\n",
    "    global MAX_LEN\n",
    "    if MAX_LEN < array.shape()[0]:\n",
    "        MAX_LEN = array.shape()[0]\n",
    "    return array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-16T11:11:07.694976Z",
     "end_time": "2023-05-16T11:11:09.214729Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "def process_data(df:pd.DataFrame):\n",
    "    #data.drop(columns=['tweet'], inplace=True)\n",
    "    print('-'*20, '\\nCleaning text')\n",
    "    df['text'] = df['text'].apply(preprocess_text)\n",
    "    print('-'*20, '\\nTokenizing text')\n",
    "    df['text'] = df['text'].apply(lambda sentence: sentence_to_words_vectors(sentence))\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-16T12:42:07.972255Z",
     "end_time": "2023-05-16T12:42:07.996695Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-16T11:11:09.246049Z",
     "end_time": "2023-05-16T11:11:09.261613Z"
    }
   },
   "outputs": [],
   "source": [
    "# add_to_dict(words, './GloVe/glove.6B/glove.6B.50d.txt')\n",
    "\n",
    "# first = True\n",
    "# count = 1\n",
    "# data_array = []\n",
    "# for chunk in pd.read_csv(DATA_PATH, chunksize=10**5,nrows=10**6):\n",
    "#     print(\"PROCESSING \" + str(count))\n",
    "#     data = process_data(chunk)\n",
    "#     data_array.append(data)\n",
    "#     count = count+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que las matrices de vectores de cada oración tienen un número diferente de filas debido a que cada oración cuenta con un número diferente de palabas. Es necesario identificar el tamaño máximo de los textos que se tienen para su \"estandarización\":"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que el tamaño máximo es 112, entonces se llevarán todas las matrices a la forma `(35, 50)`. Los valores faltantes para cada vector serán 0s en su inicio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-16T11:11:09.261613Z",
     "end_time": "2023-05-16T11:11:09.277232Z"
    }
   },
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# for i in enumerate(data_array):\n",
    "#     data_array[i] = tf.keras.utils.pad_sequences(data_array[i], maxlen=MAX_LEN, dtype='float16')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-16T11:11:09.277232Z",
     "end_time": "2023-05-16T11:11:17.187366Z"
    }
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.utils import Sequence\n",
    "import numpy as np\n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, data_array, batch_size=32, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.df_array:pd.DataFrame = data_array\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return sum(len(df) for df in self.data_array)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opcion 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-16T11:11:17.202841Z",
     "end_time": "2023-05-16T11:11:23.110616Z"
    }
   },
   "outputs": [],
   "source": [
    "add_to_dict(words, './GloVe/glove.6B/glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "start_time": "2023-05-16T12:42:14.857055Z",
     "end_time": "2023-05-16T12:42:22.186607Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import random\n",
    "\n",
    "num_lines = sum(1 for l in open(DATA_REDUCED_PATH))\n",
    "size = 10 ** 6\n",
    "skip_idx = random.sample(range(1, num_lines), num_lines - size)\n",
    "# Leemos el archivo, saltando las filas seleccionadas.\n",
    "data = pd.read_csv(DATA_REDUCED_PATH, skiprows=skip_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- \n",
      "Delete nans\n",
      "-------------------- \n",
      "Removing long sentences\n",
      "-------------------- \n",
      "Removing non eng sentences\n"
     ]
    }
   ],
   "source": [
    "data = process_data(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## División Entrenamiento-Validación-Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-13T17:08:38.751111Z",
     "start_time": "2023-05-13T17:08:38.037716Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = tf.keras.utils.pad_sequences(data['text'], maxlen=MAX_LEN, dtype='float16')\n",
    "y = data['priority']\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T01:12:40.748468Z",
     "start_time": "2023-05-01T01:12:39.779816Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test =  train_test_split(X, y, test_size=0.3, random_state=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.layers import Embedding, Lambda, LSTM, Flatten, Dense, Input, Dropout, Bidirectional, GlobalMaxPooling1D\n",
    "from keras.optimizers import Adam, RMSprop, SGD\n",
    "from kerastuner import RandomSearch, HyperParameters\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "\n",
    "def print_hyperparameters(json_data):\n",
    "    values = json_data[\"values\"]\n",
    "    \n",
    "    for key, value in sorted(values.items()):\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, min_lr=1e-5)\n",
    "cp = ModelCheckpoint('saved/', save_best_only=True)\n",
    "\n",
    "callbacks = [cp, early_stopping, reduce_lr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def build_model(hp):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(MAX_LEN, 50)))\n",
    "\n",
    "    # Hiperparámetros para LSTM 1\n",
    "    lstm_units = hp.Int(\"lstm_units_1\", min_value=128, max_value=256, step=32)\n",
    "    lstm_dropout = hp.Float(\"lstm_dropout\", min_value=0.1, max_value=0.5, step=0.1)\n",
    "    \n",
    "    model.add(Bidirectional(LSTM(lstm_units, return_sequences=True, dropout=lstm_dropout, recurrent_dropout=lstm_dropout)))\n",
    "\n",
    "    # Hiperparámetros para LSTM 2\n",
    "    lstm_units = hp.Int(\"lstm_units_2\", min_value=64, max_value=128, step=32)\n",
    "    lstm_dropout = hp.Float(\"lstm_dropout_2\", min_value=0.1, max_value=0.5, step=0.1)\n",
    "    \n",
    "    model.add(Bidirectional(LSTM(lstm_units, return_sequences=True, dropout=lstm_dropout, recurrent_dropout=lstm_dropout)))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "\n",
    "    # Hiperparámetros para capa densa 1\n",
    "    dense_units = hp.Int(\"dense_units_1\", min_value=32, max_value=128, step=32)\n",
    "    dense_dropout = hp.Float(\"dense_dropout_1\", min_value=0.1, max_value=0.5, step=0.1)\n",
    "    \n",
    "    model.add(Dense(dense_units, activation='relu'))\n",
    "    model.add(Dropout(dense_dropout))\n",
    "\n",
    "    # Hiperparámetros para capa densa 2\n",
    "    dense_units = hp.Int(\"dense_units_2\", min_value=32, max_value=128, step=32)\n",
    "    model.add(Dense(dense_units, activation='relu'))\n",
    "\n",
    "    # Salida del modelo\n",
    "    model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "    # Hiperparámetros para el optimizador (En otras pruebas se vio que Adam era el mejor)\n",
    "    learning_rate = hp.Float(\"learning_rate\", min_value=1e-5, max_value=1e-3, sampling=\"LOG\")\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Define el objeto de búsqueda aleatoria\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective=\"val_accuracy\",\n",
    "    max_trials=20,  # Número de modelos a probar\n",
    "    executions_per_trial=1,\n",
    "    directory='./saved/fine_tuned/',\n",
    "    project_name='HP_LSTM_Glove_text'\n",
    ")\n",
    "\n",
    "# Resumen de la búsqueda\n",
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=1024\n",
    "tuner.search(X_train, y_train,\n",
    "                    epochs=10,\n",
    "                    validation_split=0.1,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    callbacks=callbacks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hp_gloveLstm = tuner.get_best_hyperparameters()[0]\n",
    "\n",
    "print(\"Mejores hiperparámetros encontrados:\")\n",
    "print(best_hp_gloveLstm.get_config())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T01:14:45.215606Z",
     "start_time": "2023-05-01T01:14:43.478672Z"
    }
   },
   "outputs": [],
   "source": [
    "# from keras.models import Sequential\n",
    "# from keras.layers import Embedding, LSTM, Flatten, Dense, Input, Dropout, Bidirectional, GlobalMaxPooling1D\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Input(shape=(MAX_LEN, 50)))\n",
    "# model.add(Bidirectional(LSTM(128, return_sequences=True, dropout=0.25, recurrent_dropout=0.25)))\n",
    "# model.add(Bidirectional(LSTM(64, return_sequences=True, dropout=0.25, recurrent_dropout=0.25)))\n",
    "# model.add(GlobalMaxPooling1D())\n",
    "# model.add(Dense(128, activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(64, activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T01:26:05.211462Z",
     "start_time": "2023-05-01T01:25:53.128122Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "lstm_basic_model = load_model('saved/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T01:26:11.168631Z",
     "start_time": "2023-05-01T01:26:05.216463Z"
    }
   },
   "outputs": [],
   "source": [
    "score = lstm_basic_model.evaluate(X_test, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T01:26:11.257149Z",
     "start_time": "2023-05-01T01:26:11.092632Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Test Accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe-LSTM-DO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-30T19:38:52.074201Z",
     "start_time": "2023-04-30T19:38:50.734201Z"
    }
   },
   "outputs": [],
   "source": [
    "lstm_dropout_dense = Sequential(name='Lstm-dout-dense')\n",
    "lstm_dropout_dense.add(Input(shape=(MAX_LEN, 50)))\n",
    "lstm_dropout_dense.add(LSTM(64, return_sequences=True))\n",
    "lstm_dropout_dense.add(Dropout(0.2))\n",
    "lstm_dropout_dense.add(LSTM(32))\n",
    "lstm_dropout_dense.add(Flatten())\n",
    "lstm_dropout_dense.add(Dense(128, activation='relu'))\n",
    "lstm_dropout_dense.add(Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Input, LSTM, Dropout, Flatten, Dense\n",
    "from keras.optimizers import Adam, RMSprop, SGD\n",
    "from kerastuner import RandomSearch, HyperParameters\n",
    "\n",
    "def build_model_gloveLstmDo(hp):\n",
    "    model = Sequential(name='GloVe-LSTM-DO')\n",
    "    model.add(Input(shape=(MAX_LEN, 50)))\n",
    "    \n",
    "    # Hiperparámetros para la primera capa LSTM\n",
    "    lstm_units_1 = hp.Int(\"lstm_units_1\", min_value=32, max_value=128, step=32)\n",
    "    model.add(LSTM(lstm_units_1, return_sequences=True))\n",
    "    \n",
    "    # Hiperparámetros para Dropout\n",
    "    dropout_rate = hp.Float(\"dropout_rate\", min_value=0.1, max_value=0.5, step=0.1)\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    # Hiperparámetros para la segunda capa LSTM\n",
    "    lstm_units_2 = hp.Int(\"lstm_units_2\", min_value=16, max_value=64, step=16)\n",
    "    model.add(LSTM(lstm_units_2))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    # Hiperparámetros para la capa Densa\n",
    "    dense_units = hp.Int(\"dense_units\", min_value=32, max_value=256, step=32)\n",
    "    model.add(Dense(dense_units, activation='relu'))\n",
    "    \n",
    "    model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "    # Hiperparámetros para el optimizador\n",
    "    learning_rate = hp.Float(\"learning_rate\", min_value=1e-5, max_value=1e-3, sampling=\"LOG\")\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "gloveLstmDoTuner = RandomSearch(\n",
    "    build_model_gloveLstmDo,\n",
    "    objective=\"val_accuracy\",\n",
    "    max_trials=20,  # Número de modelos a probar\n",
    "    executions_per_trial=1,\n",
    "    directory='./saved/fine_tuned/',\n",
    "    project_name='HP_LSTM_dropout_dense'\n",
    ")\n",
    "\n",
    "# Resumen de la búsqueda\n",
    "gloveLstmDoTuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realiza la búsqueda\n",
    "gloveLstmDoTuner.search(X_train, y_train,\n",
    "             epochs=10,\n",
    "             validation_split=0.1,\n",
    "             batch_size=4096,\n",
    "             callbacks=callbacks)\n",
    "\n",
    "best_hp_gloveLstmDo = gloveLstmDoTuner.get_best_hyperparameters(num_trials=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-30T19:38:53.773316Z",
     "start_time": "2023-04-30T19:38:53.717317Z"
    }
   },
   "outputs": [],
   "source": [
    "# lstm_dropout_dense.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-30T19:49:18.525348Z",
     "start_time": "2023-04-30T19:49:18.260182Z"
    }
   },
   "outputs": [],
   "source": [
    "# lstm_dropout_dense.compile(optimizer='adam',\n",
    "#               loss='categorical_crossentropy',\n",
    "#               metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-30T19:49:02.031330Z",
     "start_time": "2023-04-30T19:49:01.983319Z"
    }
   },
   "outputs": [],
   "source": [
    "# early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-30T20:33:54.839842Z",
     "start_time": "2023-04-30T19:49:23.531267Z"
    }
   },
   "outputs": [],
   "source": [
    "# history_2 = lstm_dropout_dense.fit(X_train, y_train,\n",
    "#                     validation_split=0.2, epochs=10,\n",
    "#                     batch_size=BATCH_SIZE,\n",
    "#                     callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-30T20:35:26.123724Z",
     "start_time": "2023-04-30T20:34:57.096320Z"
    }
   },
   "outputs": [],
   "source": [
    "# score = lstm_dropout_dense.evaluate(X_test, y_test, verbose=1)\n",
    "# print('Test Accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3LSTM-DO-CNN-Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.layers import Conv1D, MaxPooling1D\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Input, LSTM, Dropout, Flatten, Dense\n",
    "# from keras.optimizers import Adam, RMSprop, SGD\n",
    "# from kerastuner import RandomSearch, HyperParameters\n",
    "\n",
    "# lstm3_do_cnn_dense = Sequential(name='3LSTM-DO-CNN-Dense')\n",
    "# lstm3_do_cnn_dense.add(Input(shape=(MAX_LEN, 50)))\n",
    "\n",
    "# #LSTM\n",
    "# lstm3_do_cnn_dense.add(LSTM(256, name='LSTM1', return_sequences=True))\n",
    "# lstm3_do_cnn_dense.add(Dropout(0.2, name='DO1'))\n",
    "\n",
    "# lstm3_do_cnn_dense.add(LSTM(128, name='LSTM2', return_sequences=True))\n",
    "# lstm3_do_cnn_dense.add(Dropout(0.2, name='DO2'))\n",
    "\n",
    "# lstm3_do_cnn_dense.add(LSTM(64, name='LSTM3', return_sequences=True))\n",
    "# lstm3_do_cnn_dense.add(Dropout(0.2, name='DO3'))\n",
    "\n",
    "# #CNN\n",
    "# lstm3_do_cnn_dense.add(Conv1D(128, kernel_size=3, strides=1, padding='same', activation='relu'))\n",
    "# lstm3_do_cnn_dense.add(MaxPooling1D(pool_size=3, strides=2, padding='same'))\n",
    "# lstm3_do_cnn_dense.add(Dropout(0.2))\n",
    "# lstm3_do_cnn_dense.add(Flatten(name='F1'))\n",
    "\n",
    "# #Fully connected\n",
    "# lstm3_do_cnn_dense.add(Dense(64, activation='relu'))\n",
    "# lstm3_do_cnn_dense.add(Dense(7, activation='softmax'))\n",
    "\n",
    "# lstm3_do_cnn_dense.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_gloveLstmDoCnn(hp):\n",
    "    model = Sequential(name='3LSTM-DO-CNN-Dense')\n",
    "    model.add(Input(shape=(MAX_LEN, 50)))\n",
    "\n",
    "    # Hiperparámetros para la primera capa LSTM\n",
    "    lstm_units_1 = hp.Int(\"lstm_units_1\", min_value=128, max_value=512, step=32)\n",
    "    model.add(LSTM(lstm_units_1, name='LSTM1', return_sequences=True))\n",
    "    lstm_dropout_1 = hp.Float(\"lstm_dropout_1\", min_value=0.1, max_value=0.5, step=0.1)\n",
    "    model.add(Dropout(lstm_dropout_1, name='DO1'))\n",
    "\n",
    "    # Hiperparámetros para la segunda capa LSTM\n",
    "    lstm_units_2 = hp.Int(\"lstm_units_2\", min_value=64, max_value=256, step=32)\n",
    "    model.add(LSTM(lstm_units_2, name='LSTM2', return_sequences=True))\n",
    "    lstm_dropout_2 = hp.Float(\"lstm_dropout_2\", min_value=0.1, max_value=0.5, step=0.1)\n",
    "    model.add(Dropout(lstm_dropout_2, name='DO2'))\n",
    "\n",
    "    # Hiperparámetros para la tercera capa LSTM\n",
    "    lstm_units_3 = hp.Int(\"lstm_units_3\", min_value=32, max_value=128, step=16)\n",
    "    model.add(LSTM(lstm_units_3, name='LSTM3', return_sequences=True))\n",
    "    lstm_dropout_3 = hp.Float(\"lstm_dropout_3\", min_value=0.1, max_value=0.5, step=0.1)\n",
    "    model.add(Dropout(lstm_dropout_3, name='DO3'))\n",
    "\n",
    "    # Hiperparámetros para la capa Conv1D\n",
    "    conv_filters = hp.Int(\"conv_filters\", min_value=32, max_value=256, step=32)\n",
    "    model.add(Conv1D(conv_filters, kernel_size=3, strides=1, padding='same', activation='relu'))\n",
    "    \n",
    "    model.add(MaxPooling1D(pool_size=3, strides=2, padding='same'))\n",
    "\n",
    "    cnn_dropout = hp.Float(\"cnn_dropout\", min_value=0.1, max_value=0.5, step=0.1)\n",
    "    model.add(Dropout(cnn_dropout))\n",
    "    \n",
    "    model.add(Flatten(name='F1'))\n",
    "\n",
    "    # Hiperparámetros para la capa Densa\n",
    "    dense_units = hp.Int(\"dense_units\", min_value=32, max_value=256, step=32)\n",
    "    model.add(Dense(dense_units, activation='relu'))\n",
    "    \n",
    "    model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "    # Hiperparámetros para el optimizador\n",
    "    learning_rate = hp.Float(\"learning_rate\", min_value=1e-5, max_value=1e-3, sampling=\"LOG\")\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = RandomSearch(\n",
    "    build_model_gloveLstmDoCnn,\n",
    "    objective=\"val_accuracy\",\n",
    "    max_trials=20,  # Número de modelos a probar\n",
    "    executions_per_trial=1,\n",
    "    directory='./saved/fine_tuned/',\n",
    "    project_name='HP_3LSTM-DO-CNN-Dense'\n",
    ")\n",
    "\n",
    "# Resumen de la búsqueda\n",
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=4096\n",
    "tuner.search(X_train, y_train,\n",
    "                    epochs=10,\n",
    "                    validation_split=0.1,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    callbacks=callbacks)\n",
    "\n",
    "best_hp_gloveLstmDoCnn = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(\"Mejores hiperparámetros encontrados:\")\n",
    "print_hyperparameters(best_hp_gloveLstmDoCnn.get_config())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MODELO 1\")\n",
    "print_hyperparameters(best_hp_gloveLstm.get_config())\n",
    "print(\" ==================================================== \")\n",
    "print(\"MODELO 2\")\n",
    "print_hyperparameters(best_hp_gloveLstmDo.get_config())\n",
    "print(\" ==================================================== \")\n",
    "print(\"MODELO 3\")\n",
    "print_hyperparameters(best_hp_gloveLstmDoCnn.get_config())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_model_1(hyper_parameters):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(MAX_LEN, 50)))\n",
    "    \n",
    "    model.add(Bidirectional(LSTM(hyper_parameters.get('lstm_units_1'), \n",
    "                                 return_sequences=True, \n",
    "                                 dropout=hyper_parameters.get('lstm_dropout'), \n",
    "                                 recurrent_dropout=hyper_parameters.get('lstm_dropout'))))\n",
    "    \n",
    "    model.add(Bidirectional(LSTM(hyper_parameters.get('lstm_units_2'), \n",
    "                                 return_sequences=True, \n",
    "                                 dropout=hyper_parameters.get('lstm_dropout_2'), \n",
    "                                 recurrent_dropout=hyper_parameters.get('lstm_dropout_2'))))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dense(hyper_parameters.get('dense_units_1'), activation='relu'))\n",
    "    model.add(Dropout(hyper_parameters.get('dense_dropout_1')))\n",
    "    model.add(Dense(hyper_parameters.get('dense_units_2'), activation='relu'))\n",
    "    model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_gloveLstm = best_model_1(best_hp_gloveLstm)\n",
    "history_gloveLstm = best_model_gloveLstm.fit(X_train, y_train,\n",
    "             epochs=30,\n",
    "             validation_split=0.2,\n",
    "             batch_size=1024,\n",
    "             callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_model_2(hp):\n",
    "    model = Sequential(name='GloVe-LSTM-DO')\n",
    "    model.add(Input(shape=(MAX_LEN, 50)))\n",
    "    \n",
    "    # Hiperparámetros para la primera capa LSTM\n",
    "    lstm_units_1 = hp.get(\"lstm_units_1\")\n",
    "    model.add(LSTM(lstm_units_1, return_sequences=True))\n",
    "    \n",
    "    # Hiperparámetros para Dropout\n",
    "    dropout_rate = hp.get(\"dropout_rate\")\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    # Hiperparámetros para la segunda capa LSTM\n",
    "    lstm_units_2 = hp.get(\"lstm_units_2\")\n",
    "    model.add(LSTM(lstm_units_2))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    # Hiperparámetros para la capa Densa\n",
    "    dense_units = hp.get(\"dense_units\")\n",
    "    model.add(Dense(dense_units, activation='relu'))\n",
    "    \n",
    "    model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "    # Hiperparámetros para el optimizador\n",
    "    learning_rate = hp.get(\"learning_rate\")\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_gloveLstmDo = best_model_2(best_hp_gloveLstmDo)\n",
    "history_gloveLstmDo = best_model_gloveLstmDo.fit(X_train, y_train,\n",
    "             epochs=30,\n",
    "             validation_split=0.2,\n",
    "             batch_size=4093,\n",
    "             callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_model_3(hp):\n",
    "    model = Sequential(name='3LSTM-DO-CNN-Dense')\n",
    "    model.add(Input(shape=(MAX_LEN, 50)))\n",
    "\n",
    "    # Hiperparámetros para la primera capa LSTM\n",
    "    lstm_units_1 = hp.get(\"lstm_units_1\")\n",
    "    model.add(LSTM(lstm_units_1, name='LSTM1', return_sequences=True))\n",
    "    lstm_dropout_1 = hp.get(\"lstm_dropout_1\")\n",
    "    model.add(Dropout(lstm_dropout_1, name='DO1'))\n",
    "\n",
    "    # Hiperparámetros para la segunda capa LSTM\n",
    "    lstm_units_2 = hp.get(\"lstm_units_2\")\n",
    "    model.add(LSTM(lstm_units_2, name='LSTM2', return_sequences=True))\n",
    "    lstm_dropout_2 = hp.get(\"lstm_dropout_2\")\n",
    "    model.add(Dropout(lstm_dropout_2, name='DO2'))\n",
    "\n",
    "    # Hiperparámetros para la tercera capa LSTM\n",
    "    lstm_units_3 = hp.get(\"lstm_units_3\")\n",
    "    model.add(LSTM(lstm_units_3, name='LSTM3', return_sequences=True))\n",
    "    lstm_dropout_3 = hp.get(\"lstm_dropout_3\")\n",
    "    model.add(Dropout(lstm_dropout_3, name='DO3'))\n",
    "\n",
    "    # Hiperparámetros para la capa Conv1D\n",
    "    conv_filters = hp.get(\"conv_filters\")\n",
    "    model.add(Conv1D(conv_filters, kernel_size=3, strides=1, padding='same', activation='relu'))\n",
    "    \n",
    "    model.add(MaxPooling1D(pool_size=3, strides=2, padding='same'))\n",
    "\n",
    "    cnn_dropout = hp.get(\"cnn_dropout\")\n",
    "    model.add(Dropout(cnn_dropout))\n",
    "    \n",
    "    model.add(Flatten(name='F1'))\n",
    "\n",
    "    # Hiperparámetros para la capa Densa\n",
    "    dense_units = hp.get(\"dense_units\")\n",
    "    model.add(Dense(dense_units, activation='relu'))\n",
    "    \n",
    "    model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "    # Hiperparámetros para el optimizador\n",
    "    learning_rate = hp.get(\"learning_rate\")\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_gloveLstmDoCnn = best_model_3(best_hp_gloveLstmDoCnn)\n",
    "history_gloveLstmDoCnn = best_model_gloveLstmDoCnn.fit(X_train, y_train,\n",
    "             epochs=30,\n",
    "             validation_split=0.2,\n",
    "             batch_size=4093,\n",
    "             callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Glove LSTM\")\n",
    "scoreGloveLstm = best_model_gloveLstm.evaluate(X_test, y_test, verbose=1)\n",
    "print('Test Accuracy:', scoreGloveLstm[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Glove LSTM Do\")\n",
    "scoreGloveLstmDo = best_model_gloveLstmDo.evaluate(X_test, y_test, verbose=1)\n",
    "print('Test Accuracy:', scoreGloveLstmDo[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Glove LSTM Do Cnn\")\n",
    "scoreGloveLstmDoCnn = best_model_gloveLstmDoCnn.evaluate(X_test, y_test, verbose=1)\n",
    "print('Test Accuracy:', scoreGloveLstmDoCnn[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Glove LSTM\")\n",
    "best_model_gloveLstm.save('./saved/GloveLSTM.h5')\n",
    "best_model_gloveLstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Glove LSTM Do\")\n",
    "best_model_gloveLstmDo.save('./saved/GloveLSTMDo.h5')\n",
    "best_model_gloveLstmDo.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Glove LSTM Do Cnn\")\n",
    "best_model_gloveLstmDoCnn.save('./saved/GloveLSTMDoCNN.h5')\n",
    "best_model_gloveLstmDoCnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graficas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Val vs Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def plot_val_vs_loss(history):\n",
    "    num_epochs = len(history.epoch)\n",
    "\n",
    "    epochs = [i for i in range(num_epochs)]\n",
    "    fig , ax = plt.subplots(1,2)\n",
    "    train_acc = history.history['accuracy']\n",
    "    train_loss = history.history['loss']\n",
    "    test_acc = history.history['val_accuracy']\n",
    "    test_loss = history.history['val_loss']\n",
    "\n",
    "    fig.set_size_inches(20,6)\n",
    "    ax[0].plot(epochs , train_loss , label = 'Training Loss')\n",
    "    ax[0].plot(epochs , test_loss , label = 'Testing Loss')\n",
    "    ax[0].set_title('Training & Testing Loss')\n",
    "    ax[0].legend()\n",
    "    ax[0].set_xlabel(\"Epochs\")\n",
    "\n",
    "    ax[1].plot(epochs , train_acc , label = 'Training Accuracy')\n",
    "    ax[1].plot(epochs , test_acc , label = 'Testing Accuracy')\n",
    "    ax[1].set_title('Training & Testing Accuracy')\n",
    "    ax[1].legend()\n",
    "    ax[1].set_xlabel(\"Epochs\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_val_vs_loss(history_gloveLstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_val_vs_loss(history_gloveLstmDo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_val_vs_loss(history_gloveLstmDoCnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_GloveLstm = best_model_gloveLstm.predict(X_test)\n",
    "y_pred_GloveLstmDo = best_model_gloveLstmDo.predict(X_test)\n",
    "y_pred_GloveLstmDoCnn = best_model_gloveLstmDoCnn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_GloveLstm[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_np =  np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_heatmap(y_pred, y_test, labels):\n",
    "    # Realizar predicciones\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    y_true_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "    # Calcular la matriz de confusión\n",
    "    cm = confusion_matrix(y_true_classes, y_pred_classes)\n",
    "\n",
    "    # Crear el heatmap utilizando Seaborn\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='coolwarm', cbar=False, xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()\n",
    "\n",
    "labels = ['Clase 1', 'Clase 2', 'Clase 3', 'Clase 4', 'Clase 5', 'Clase 6', 'Clase 7']\n",
    "plot_heatmap(y_pred_GloveLstm, y_test_np, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_heatmap(y_pred_GloveLstmDo, y_test_np, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_heatmap(y_pred_GloveLstmDoCnn, y_test_np, labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
