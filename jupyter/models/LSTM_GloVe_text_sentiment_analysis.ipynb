{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM-GloVe Model for Sentiment Analysis in text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este notebook se describe el proceso de carga, preprocesamiento, embedding, construcción y entrenamiento de un modelo que emplea LSTM y GloVe\n",
    "para el set de datos de texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-16T11:11:02.078805Z",
     "start_time": "2023-05-16T11:11:01.750681Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA_PATH = '../../data/text/cleaned/final.csv'\n",
    "DATA_REDUCED_PATH = '../../data/text/cleaned/reduced_final_eng_clean.csv'\n",
    "DATA_OUTPUT_PATH = '../../data/text/cleaned/preprocessed/final.pkl'\n",
    "CHUNK_SIZE = 10**6\n",
    "COLUMNS = ['text','priority']\n",
    "words = {}\n",
    "MAX_LEN = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRE PROCESAMIENTO DE DATOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocesamiento\n",
    "Es necesario remover de nuestros datos información irrelevante como etiquetas, puntución, números y caracteres especiales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-16T11:11:02.110062Z",
     "start_time": "2023-05-16T11:11:01.797577Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "TAG_RE = re.compile(r'@[^> ]+')\n",
    "\n",
    "def remove_at_sign(sentence: str):\n",
    "    '''\n",
    "    Replaces '@' from and input string for an empty space\n",
    "    :param sentence: String that contains @\n",
    "    :return: sentence without @\n",
    "    '''\n",
    "\n",
    "    return TAG_RE.sub('', sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-16T11:11:03.412785Z",
     "start_time": "2023-05-16T11:11:01.813183Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langid'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlangid\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdeep_translator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GoogleTranslator\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtranslate_sentence\u001b[39m(sentence: \u001b[38;5;28mstr\u001b[39m):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'langid'"
     ]
    }
   ],
   "source": [
    "import langid\n",
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "def translate_sentence(sentence: str):\n",
    "    \"\"\"\n",
    "    Translate a sentence to english if it's in different language.\n",
    "    :param sentence: The string/sentence to translate\n",
    "    :return: The original sentence in english\n",
    "    \"\"\"\n",
    "    lang = langid.classify(sentence)[0]\n",
    "    if lang != 'en' and len(sentence) < 5000:\n",
    "        sentence = GoogleTranslator(source='auto').translate(sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-16T11:11:07.585600Z",
     "start_time": "2023-05-16T11:11:03.412785Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-16T14:26:43.714184Z",
     "start_time": "2023-05-16T14:26:43.682808Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def preprocess_text(sentence: str):\n",
    "    '''\n",
    "    Cleans up a sentence leaving only 2 or more non-stopwords composed of upper and lowercase\n",
    "    :param sentence: String to be cleaned\n",
    "    :return: sentence without numbers, special chars and long stopwords\n",
    "    '''\n",
    "\n",
    "    cleaned_sentence = sentence.lower()\n",
    "    cleaned_sentence = remove_at_sign(cleaned_sentence)\n",
    "    cleaned_sentence = re.sub('[^a-zA-Z]', ' ', cleaned_sentence)\n",
    "    cleaned_sentence = re.sub('\\s+[a-zA-Z]\\s', ' ', cleaned_sentence)\n",
    "    cleaned_sentence = re.sub('\\s+', ' ', cleaned_sentence)\n",
    "\n",
    "    #Translate\n",
    "    #cleaned_sentence = translate_sentence(cleaned_sentence)\n",
    "\n",
    "    #Removal of stopwords\n",
    "    pattern = re.compile(r'\\b(' + r'|'.join(stopwords.words('english')) + r')\\b\\s')\n",
    "    cleaned_sentence = pattern.sub('', cleaned_sentence)\n",
    "\n",
    "    return cleaned_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-16T11:11:07.679357Z",
     "start_time": "2023-05-16T11:11:07.601185Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def add_to_dict(dictionary, filename):\n",
    "    \"\"\"\n",
    "    Agrega elementos a un diccionario a partir de un archivo.\n",
    "\n",
    "    :param dictionary: Diccionario al que se agregarán los elementos.\n",
    "    :type dictionary: dict\n",
    "    :param filename: Nombre del archivo que contiene los elementos.\n",
    "    :type filename: str\n",
    "    \"\"\"\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            line = line.split(' ')\n",
    "\n",
    "            try:\n",
    "                dictionary[line[0]] = np.array(line[1:], dtype=float)\n",
    "            except:\n",
    "                continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenización y Lematización\n",
    "Una vez cargada la información de los tokens de GloVe se procede a tokenizar y lematizar cada\n",
    "una de las oraciones en nuestro set de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-16T11:11:07.679357Z",
     "start_time": "2023-05-16T11:11:07.616819Z"
    }
   },
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-16T11:11:07.679357Z",
     "start_time": "2023-05-16T11:11:07.663698Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def sentence_to_token_list(sentence: str):\n",
    "    \"\"\"\n",
    "    Convierte una oración en una lista de tokens útiles.\n",
    "\n",
    "    :param sentence: Oración a convertir.\n",
    "    :type sentence: str\n",
    "    :return: Lista de tokens útiles.\n",
    "    :rtype: list\n",
    "    \"\"\"\n",
    "    # Tokenización de la oración\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "\n",
    "    # Lemmatización de los tokens\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    # Selección de los tokens útiles\n",
    "    useful_tokens = [token for token in lemmatized_tokens if token in words]\n",
    "\n",
    "    return useful_tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el token anterior, el cual sabemos se puede representar por medio de uno de los tokens almacenados en `words`, entonces pasamos a la representación de estos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-16T11:11:07.757445Z",
     "start_time": "2023-05-16T11:11:07.679357Z"
    }
   },
   "outputs": [],
   "source": [
    "def sentence_to_words_vectors(sentence: str, word_dict=words):\n",
    "    \"\"\"\n",
    "    Convierte una oración en una matriz de vectores de palabras.\n",
    "\n",
    "    :param sentence: Oración a convertir.\n",
    "    :type sentence: str\n",
    "    :param word_dict: Diccionario de palabras y vectores. Por defecto, utiliza 'words'.\n",
    "    :type word_dict: dict\n",
    "    :return: Matriz de vectores de palabras.\n",
    "    :rtype: numpy.ndarray\n",
    "    \"\"\"\n",
    "    # Obtener los tokens procesados de la oración\n",
    "    processed_tokens = sentence_to_token_list(sentence)\n",
    "\n",
    "    # Obtener los vectores correspondientes a los tokens procesados\n",
    "    vectors = []\n",
    "    for token in processed_tokens:\n",
    "        if token in word_dict:\n",
    "            token_vector = word_dict[token]\n",
    "            vectors.append(token_vector)\n",
    "\n",
    "    # Convertir la lista de vectores en una matriz NumPy\n",
    "    array = np.array(vectors, dtype=float)\n",
    "\n",
    "    # Actualizar la variable global MAX_LEN si es necesario\n",
    "    global MAX_LEN\n",
    "    if MAX_LEN < array.shape[0]:\n",
    "        MAX_LEN = array.shape[0]\n",
    "\n",
    "    return array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-16T11:11:09.214729Z",
     "start_time": "2023-05-16T11:11:07.694976Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-16T12:42:07.996695Z",
     "start_time": "2023-05-16T12:42:07.972255Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def process_data(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Procesa los datos en un DataFrame dado.\n",
    "\n",
    "    :param df: DataFrame con los datos a procesar.\n",
    "    :type df: pd.DataFrame\n",
    "    :return: DataFrame procesado.\n",
    "    :rtype: pd.DataFrame\n",
    "    \"\"\"\n",
    "    # Eliminar la columna 'tweet' (comentada en el código original)\n",
    "    # data.drop(columns=['tweet'], inplace=True)\n",
    "\n",
    "    # Limpiar el texto en la columna 'text'\n",
    "    print('-'*20, '\\nCleaning text')\n",
    "    df['text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "    # Tokenizar el texto en la columna 'text' y convertirlo en vectores de palabras\n",
    "    print('-'*20, '\\nTokenizing text')\n",
    "    df['text'] = df['text'].apply(lambda sentence: sentence_to_words_vectors(sentence))\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-16T11:11:09.261613Z",
     "start_time": "2023-05-16T11:11:09.246049Z"
    }
   },
   "outputs": [],
   "source": [
    "# add_to_dict(words, './GloVe/glove.6B/glove.6B.50d.txt')\n",
    "\n",
    "# first = True\n",
    "# count = 1\n",
    "# data_array = []\n",
    "# for chunk in pd.read_csv(DATA_PATH, chunksize=10**5,nrows=10**6):\n",
    "#     print(\"PROCESSING \" + str(count))\n",
    "#     data = process_data(chunk)\n",
    "#     data_array.append(data)\n",
    "#     count = count+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que las matrices de vectores de cada oración tienen un número diferente de filas debido a que cada oración cuenta con un número diferente de palabas. Es necesario identificar el tamaño máximo de los textos que se tienen para su \"estandarización\":"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que el tamaño máximo es 112, entonces se llevarán todas las matrices a la forma `(35, 50)`. Los valores faltantes para cada vector serán 0s en su inicio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-16T11:11:09.277232Z",
     "start_time": "2023-05-16T11:11:09.261613Z"
    }
   },
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# for i in enumerate(data_array):\n",
    "#     data_array[i] = tf.keras.utils.pad_sequences(data_array[i], maxlen=MAX_LEN, dtype='float16')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-16T11:11:17.187366Z",
     "start_time": "2023-05-16T11:11:09.277232Z"
    }
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.utils import Sequence\n",
    "import numpy as np\n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, data_array, batch_size=32, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.df_array:pd.DataFrame = data_array\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return sum(len(df) for df in self.data_array)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opcion 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-16T11:11:23.110616Z",
     "start_time": "2023-05-16T11:11:17.202841Z"
    }
   },
   "outputs": [],
   "source": [
    "add_to_dict(words, './GloVe/glove.6B/glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-16T12:42:22.186607Z",
     "start_time": "2023-05-16T12:42:14.857055Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import random\n",
    "\n",
    "num_lines = sum(1 for l in open(DATA_REDUCED_PATH))\n",
    "size = 10 ** 6\n",
    "skip_idx = random.sample(range(1, num_lines), num_lines - size)\n",
    "# Leemos el archivo, saltando las filas seleccionadas.\n",
    "data = pd.read_csv(DATA_REDUCED_PATH, skiprows=skip_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "data = process_data(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## División Entrenamiento-Validación-Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-13T17:08:38.751111Z",
     "start_time": "2023-05-13T17:08:38.037716Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = tf.keras.utils.pad_sequences(data['text'], maxlen=MAX_LEN, dtype='float16')\n",
    "y = data['priority']\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T01:12:40.748468Z",
     "start_time": "2023-05-01T01:12:39.779816Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_rest, y_train, y_rest =  train_test_split(X, y, test_size=0.3, random_state=3)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_rest, y_rest, test_size=0.5, random_state=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.layers import Embedding, Lambda, LSTM, Flatten, Dense, Input, Dropout, Bidirectional, GlobalMaxPooling1D\n",
    "from keras.optimizers import Adam, RMSprop, SGD\n",
    "from kerastuner import RandomSearch, HyperParameters\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "\n",
    "def print_hyperparameters(json_data):\n",
    "    \"\"\"\n",
    "    Imprime los hiperparámetros contenidos en un objeto JSON.\n",
    "\n",
    "    :param json_data: Objeto JSON que contiene los hiperparámetros.\n",
    "    :type json_data: dict\n",
    "    \"\"\"\n",
    "    values = json_data[\"values\"]\n",
    "\n",
    "    for key, value in sorted(values.items()):\n",
    "        print(f\"{key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, min_lr=1e-5)\n",
    "cp = ModelCheckpoint('saved/', save_best_only=True)\n",
    "\n",
    "callbacks = [cp, early_stopping, reduce_lr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "\n",
    "    mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "    with mirrored_strategy.scope():\n",
    "    \n",
    "        model = Sequential()\n",
    "        model.add(Input(shape=(MAX_LEN, 50)))\n",
    "\n",
    "        # Hiperparámetros para LSTM 1\n",
    "        lstm_units = hp.Int(\"lstm_units_1\", min_value=128, max_value=256, step=32)\n",
    "        lstm_dropout = hp.Float(\"lstm_dropout\", min_value=0.1, max_value=0.5, step=0.1)\n",
    "\n",
    "        model.add(Bidirectional(LSTM(lstm_units, return_sequences=True, dropout=lstm_dropout, recurrent_dropout=lstm_dropout)))\n",
    "\n",
    "        # Hiperparámetros para LSTM 2\n",
    "        lstm_units = hp.Int(\"lstm_units_2\", min_value=64, max_value=128, step=32)\n",
    "        lstm_dropout = hp.Float(\"lstm_dropout_2\", min_value=0.1, max_value=0.5, step=0.1)\n",
    "\n",
    "        model.add(Bidirectional(LSTM(lstm_units, return_sequences=True, dropout=lstm_dropout, recurrent_dropout=lstm_dropout)))\n",
    "        model.add(GlobalMaxPooling1D())\n",
    "\n",
    "        # Hiperparámetros para capa densa 1\n",
    "        dense_units = hp.Int(\"dense_units_1\", min_value=32, max_value=128, step=32)\n",
    "        dense_dropout = hp.Float(\"dense_dropout_1\", min_value=0.1, max_value=0.5, step=0.1)\n",
    "\n",
    "        model.add(Dense(dense_units, activation='relu'))\n",
    "        model.add(Dropout(dense_dropout))\n",
    "\n",
    "        # Hiperparámetros para capa densa 2\n",
    "        dense_units = hp.Int(\"dense_units_2\", min_value=32, max_value=128, step=32)\n",
    "        model.add(Dense(dense_units, activation='relu'))\n",
    "\n",
    "        # Salida del modelo\n",
    "        model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "        # Hiperparámetros para el optimizador (En otras pruebas se vio que Adam era el mejor)\n",
    "        learning_rate = hp.Float(\"learning_rate\", min_value=1e-5, max_value=1e-3, sampling=\"LOG\")\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Define el objeto de búsqueda aleatoria\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective=\"val_accuracy\",\n",
    "    max_trials=20,  # Número de modelos a probar\n",
    "    executions_per_trial=1,\n",
    "    directory='./saved/fine_tuned/',\n",
    "    project_name='HP_LSTM_Glove_text'\n",
    ")\n",
    "\n",
    "# Resumen de la búsqueda\n",
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=1024\n",
    "tuner.search(X_train, y_train,\n",
    "                    epochs=10,\n",
    "                    validation_split=0.1,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    callbacks=callbacks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hp_gloveLstm = tuner.get_best_hyperparameters()[0]\n",
    "\n",
    "print(\"Mejores hiperparámetros encontrados:\")\n",
    "print(best_hp_gloveLstm.get_config())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T01:14:45.215606Z",
     "start_time": "2023-05-01T01:14:43.478672Z"
    }
   },
   "outputs": [],
   "source": [
    "# from keras.models import Sequential\n",
    "# from keras.layers import Embedding, LSTM, Flatten, Dense, Input, Dropout, Bidirectional, GlobalMaxPooling1D\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Input(shape=(MAX_LEN, 50)))\n",
    "# model.add(Bidirectional(LSTM(128, return_sequences=True, dropout=0.25, recurrent_dropout=0.25)))\n",
    "# model.add(Bidirectional(LSTM(64, return_sequences=True, dropout=0.25, recurrent_dropout=0.25)))\n",
    "# model.add(GlobalMaxPooling1D())\n",
    "# model.add(Dense(128, activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(64, activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T01:26:05.211462Z",
     "start_time": "2023-05-01T01:25:53.128122Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "lstm_basic_model = load_model('saved/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T01:26:11.168631Z",
     "start_time": "2023-05-01T01:26:05.216463Z"
    }
   },
   "outputs": [],
   "source": [
    "score = lstm_basic_model.evaluate(X_test, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T01:26:11.257149Z",
     "start_time": "2023-05-01T01:26:11.092632Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Test Accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe-LSTM-DO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-30T19:38:52.074201Z",
     "start_time": "2023-04-30T19:38:50.734201Z"
    }
   },
   "outputs": [],
   "source": [
    "mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "with mirrored_strategy.scope():\n",
    "    lstm_dropout_dense = Sequential(name='Lstm-dout-dense')\n",
    "    lstm_dropout_dense.add(Input(shape=(MAX_LEN, 50)))\n",
    "    lstm_dropout_dense.add(LSTM(64, return_sequences=True))\n",
    "    lstm_dropout_dense.add(Dropout(0.2))\n",
    "    lstm_dropout_dense.add(LSTM(32))\n",
    "    lstm_dropout_dense.add(Flatten())\n",
    "    lstm_dropout_dense.add(Dense(128, activation='relu'))\n",
    "    lstm_dropout_dense.add(Dense(7, activation='softmax'))\n",
    "\n",
    "lstm_dropout_dense.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=1024\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "lstm_dropout_dense.fit(X, y, epochs=10, batch_size=BATCH_SIZE, callbacks=callbacks, validation_data=(X_val, y_val), validation_batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Input, LSTM, Dropout, Flatten, Dense\n",
    "from keras.optimizers import Adam, RMSprop, SGD\n",
    "from kerastuner import RandomSearch, HyperParameters\n",
    "\n",
    "def build_model_gloveLstmDo(hp):\n",
    "    \"\"\"\n",
    "    Construye un modelo secuencial con capas LSTM, Dropout y Densa, con hiperparámetros ajustables.\n",
    "\n",
    "    :param hp: Objeto HyperParameters que contiene los hiperparámetros de la búsqueda.\n",
    "    :type hp: tensorflow.keras.optimizers.HyperParameters\n",
    "    :return: Modelo construido.\n",
    "    :rtype: tensorflow.keras.models.Sequential\n",
    "    \"\"\"\n",
    "    model = Sequential(name='GloVe-LSTM-DO')\n",
    "    model.add(Input(shape=(MAX_LEN, 50)))\n",
    "\n",
    "    # Hiperparámetros para la primera capa LSTM\n",
    "    lstm_units_1 = hp.Int(\"lstm_units_1\", min_value=32, max_value=128, step=32)\n",
    "    model.add(LSTM(lstm_units_1, return_sequences=True))\n",
    "\n",
    "    # Hiperparámetros para Dropout\n",
    "    dropout_rate = hp.Float(\"dropout_rate\", min_value=0.1, max_value=0.5, step=0.1)\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    # Hiperparámetros para la segunda capa LSTM\n",
    "    lstm_units_2 = hp.Int(\"lstm_units_2\", min_value=16, max_value=64, step=16)\n",
    "    model.add(LSTM(lstm_units_2))\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # Hiperparámetros para la capa Densa\n",
    "    dense_units = hp.Int(\"dense_units\", min_value=32, max_value=256, step=32)\n",
    "    model.add(Dense(dense_units, activation='relu'))\n",
    "\n",
    "    model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "    # Hiperparámetros para el optimizador\n",
    "    learning_rate = hp.Float(\"learning_rate\", min_value=1e-5, max_value=1e-3, sampling=\"LOG\")\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "gloveLstmDoTuner = RandomSearch(\n",
    "    build_model_gloveLstmDo,\n",
    "    objective=\"val_accuracy\",\n",
    "    max_trials=20,  # Número de modelos a probar\n",
    "    executions_per_trial=1,\n",
    "    directory='./saved/fine_tuned/',\n",
    "    project_name='HP_LSTM_dropout_dense'\n",
    ")\n",
    "\n",
    "# Resumen de la búsqueda\n",
    "gloveLstmDoTuner.search_space_summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realiza la búsqueda\n",
    "gloveLstmDoTuner.search(X_train, y_train,\n",
    "             epochs=10,\n",
    "             validation_split=0.1,\n",
    "             batch_size=4096,\n",
    "             callbacks=callbacks)\n",
    "\n",
    "best_hp_gloveLstmDo = gloveLstmDoTuner.get_best_hyperparameters(num_trials=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-30T19:38:53.773316Z",
     "start_time": "2023-04-30T19:38:53.717317Z"
    }
   },
   "outputs": [],
   "source": [
    "# lstm_dropout_dense.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-30T19:49:18.525348Z",
     "start_time": "2023-04-30T19:49:18.260182Z"
    }
   },
   "outputs": [],
   "source": [
    "# lstm_dropout_dense.compile(optimizer='adam',\n",
    "#               loss='categorical_crossentropy',\n",
    "#               metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-30T19:49:02.031330Z",
     "start_time": "2023-04-30T19:49:01.983319Z"
    }
   },
   "outputs": [],
   "source": [
    "# early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-30T20:33:54.839842Z",
     "start_time": "2023-04-30T19:49:23.531267Z"
    }
   },
   "outputs": [],
   "source": [
    "# history_2 = lstm_dropout_dense.fit(X_train, y_train,\n",
    "#                     validation_split=0.2, epochs=10,\n",
    "#                     batch_size=BATCH_SIZE,\n",
    "#                     callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-30T20:35:26.123724Z",
     "start_time": "2023-04-30T20:34:57.096320Z"
    }
   },
   "outputs": [],
   "source": [
    "# score = lstm_dropout_dense.evaluate(X_test, y_test, verbose=1)\n",
    "# print('Test Accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3LSTM-DO-CNN-Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.layers import Conv1D, MaxPooling1D\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Input, LSTM, Dropout, Flatten, Dense\n",
    "# from keras.optimizers import Adam, RMSprop, SGD\n",
    "# from kerastuner import RandomSearch, HyperParameters\n",
    "\n",
    "# lstm3_do_cnn_dense = Sequential(name='3LSTM-DO-CNN-Dense')\n",
    "# lstm3_do_cnn_dense.add(Input(shape=(MAX_LEN, 50)))\n",
    "\n",
    "# #LSTM\n",
    "# lstm3_do_cnn_dense.add(LSTM(256, name='LSTM1', return_sequences=True))\n",
    "# lstm3_do_cnn_dense.add(Dropout(0.2, name='DO1'))\n",
    "\n",
    "# lstm3_do_cnn_dense.add(LSTM(128, name='LSTM2', return_sequences=True))\n",
    "# lstm3_do_cnn_dense.add(Dropout(0.2, name='DO2'))\n",
    "\n",
    "# lstm3_do_cnn_dense.add(LSTM(64, name='LSTM3', return_sequences=True))\n",
    "# lstm3_do_cnn_dense.add(Dropout(0.2, name='DO3'))\n",
    "\n",
    "# #CNN\n",
    "# lstm3_do_cnn_dense.add(Conv1D(128, kernel_size=3, strides=1, padding='same', activation='relu'))\n",
    "# lstm3_do_cnn_dense.add(MaxPooling1D(pool_size=3, strides=2, padding='same'))\n",
    "# lstm3_do_cnn_dense.add(Dropout(0.2))\n",
    "# lstm3_do_cnn_dense.add(Flatten(name='F1'))\n",
    "\n",
    "# #Fully connected\n",
    "# lstm3_do_cnn_dense.add(Dense(64, activation='relu'))\n",
    "# lstm3_do_cnn_dense.add(Dense(7, activation='softmax'))\n",
    "\n",
    "# lstm3_do_cnn_dense.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_gloveLstmDoCnn(hp):\n",
    "    model = Sequential(name='3LSTM-DO-CNN-Dense')\n",
    "    model.add(Input(shape=(MAX_LEN, 50)))\n",
    "\n",
    "     \"\"\"\n",
    "    Construye un modelo secuencial con capas LSTM, Dropout, Conv1D y Densa, con hiperparámetros ajustables.\n",
    "\n",
    "    :param hp: Objeto HyperParameters que contiene los hiperparámetros de la búsqueda.\n",
    "    :type hp: tensorflow.keras.optimizers.HyperParameters\n",
    "    :return: Modelo construido.\n",
    "    :rtype: tensorflow.keras.models.Sequential\n",
    "    \"\"\"\n",
    "        \n",
    "    # Hiperparámetros para la primera capa LSTM\n",
    "    lstm_units_1 = hp.Int(\"lstm_units_1\", min_value=128, max_value=512, step=32)\n",
    "    model.add(LSTM(lstm_units_1, name='LSTM1', return_sequences=True))\n",
    "    lstm_dropout_1 = hp.Float(\"lstm_dropout_1\", min_value=0.1, max_value=0.5, step=0.1)\n",
    "    model.add(Dropout(lstm_dropout_1, name='DO1'))\n",
    "\n",
    "    # Hiperparámetros para la segunda capa LSTM\n",
    "    lstm_units_2 = hp.Int(\"lstm_units_2\", min_value=64, max_value=256, step=32)\n",
    "    model.add(LSTM(lstm_units_2, name='LSTM2', return_sequences=True))\n",
    "    lstm_dropout_2 = hp.Float(\"lstm_dropout_2\", min_value=0.1, max_value=0.5, step=0.1)\n",
    "    model.add(Dropout(lstm_dropout_2, name='DO2'))\n",
    "\n",
    "    # Hiperparámetros para la tercera capa LSTM\n",
    "    lstm_units_3 = hp.Int(\"lstm_units_3\", min_value=32, max_value=128, step=16)\n",
    "    model.add(LSTM(lstm_units_3, name='LSTM3', return_sequences=True))\n",
    "    lstm_dropout_3 = hp.Float(\"lstm_dropout_3\", min_value=0.1, max_value=0.5, step=0.1)\n",
    "    model.add(Dropout(lstm_dropout_3, name='DO3'))\n",
    "\n",
    "    # Hiperparámetros para la capa Conv1D\n",
    "    conv_filters = hp.Int(\"conv_filters\", min_value=32, max_value=256, step=32)\n",
    "    model.add(Conv1D(conv_filters, kernel_size=3, strides=1, padding='same', activation='relu'))\n",
    "    \n",
    "    model.add(MaxPooling1D(pool_size=3, strides=2, padding='same'))\n",
    "\n",
    "    cnn_dropout = hp.Float(\"cnn_dropout\", min_value=0.1, max_value=0.5, step=0.1)\n",
    "    model.add(Dropout(cnn_dropout))\n",
    "    \n",
    "    model.add(Flatten(name='F1'))\n",
    "\n",
    "    # Hiperparámetros para la capa Densa\n",
    "    dense_units = hp.Int(\"dense_units\", min_value=32, max_value=256, step=32)\n",
    "    model.add(Dense(dense_units, activation='relu'))\n",
    "    \n",
    "    model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "    # Hiperparámetros para el optimizador\n",
    "    learning_rate = hp.Float(\"learning_rate\", min_value=1e-5, max_value=1e-3, sampling=\"LOG\")\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = RandomSearch(\n",
    "    build_model_gloveLstmDoCnn,\n",
    "    objective=\"val_accuracy\",\n",
    "    max_trials=20,  # Número de modelos a probar\n",
    "    executions_per_trial=1,\n",
    "    directory='./saved/fine_tuned/',\n",
    "    project_name='HP_3LSTM-DO-CNN-Dense'\n",
    ")\n",
    "\n",
    "# Resumen de la búsqueda\n",
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=4096\n",
    "tuner.search(X_train, y_train,\n",
    "                    epochs=10,\n",
    "                    validation_split=0.1,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    callbacks=callbacks)\n",
    "\n",
    "best_hp_gloveLstmDoCnn = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(\"Mejores hiperparámetros encontrados:\")\n",
    "print_hyperparameters(best_hp_gloveLstmDoCnn.get_config())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MODELO 1\")\n",
    "print_hyperparameters(best_hp_gloveLstm.get_config())\n",
    "print(\" ==================================================== \")\n",
    "print(\"MODELO 2\")\n",
    "print_hyperparameters(best_hp_gloveLstmDo.get_config())\n",
    "print(\" ==================================================== \")\n",
    "print(\"MODELO 3\")\n",
    "print_hyperparameters(best_hp_gloveLstmDoCnn.get_config())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_model_1(hyper_parameters):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(MAX_LEN, 50)))\n",
    "    \n",
    "    model.add(Bidirectional(LSTM(hyper_parameters.get('lstm_units_1'), \n",
    "                                 return_sequences=True, \n",
    "                                 dropout=hyper_parameters.get('lstm_dropout'), \n",
    "                                 recurrent_dropout=hyper_parameters.get('lstm_dropout'))))\n",
    "    \n",
    "    model.add(Bidirectional(LSTM(hyper_parameters.get('lstm_units_2'), \n",
    "                                 return_sequences=True, \n",
    "                                 dropout=hyper_parameters.get('lstm_dropout_2'), \n",
    "                                 recurrent_dropout=hyper_parameters.get('lstm_dropout_2'))))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dense(hyper_parameters.get('dense_units_1'), activation='relu'))\n",
    "    model.add(Dropout(hyper_parameters.get('dense_dropout_1')))\n",
    "    model.add(Dense(hyper_parameters.get('dense_units_2'), activation='relu'))\n",
    "    model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_gloveLstm = best_model_1(best_hp_gloveLstm)\n",
    "history_gloveLstm = best_model_gloveLstm.fit(X_train, y_train,\n",
    "             epochs=30,\n",
    "             validation_split=0.2,\n",
    "             batch_size=1024,\n",
    "             callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_model_2(hp):\n",
    "    model = Sequential(name='GloVe-LSTM-DO')\n",
    "    model.add(Input(shape=(MAX_LEN, 50)))\n",
    "    \n",
    "      \"\"\"\n",
    "    Construye un modelo secuencial con capas LSTM, Dropout y Densa, utilizando los mejores hiperparámetros encontrados.\n",
    "\n",
    "    :param hp: Objeto HyperParameters que contiene los mejores hiperparámetros encontrados.\n",
    "    :type hp: dict\n",
    "    :return: Modelo construido.\n",
    "    :rtype: tensorflow.keras.models.Sequential\n",
    "    \"\"\"\n",
    "    \n",
    "    # Hiperparámetros para la primera capa LSTM\n",
    "    lstm_units_1 = hp.get(\"lstm_units_1\")\n",
    "    model.add(LSTM(lstm_units_1, return_sequences=True))\n",
    "    \n",
    "    # Hiperparámetros para Dropout\n",
    "    dropout_rate = hp.get(\"dropout_rate\")\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    # Hiperparámetros para la segunda capa LSTM\n",
    "    lstm_units_2 = hp.get(\"lstm_units_2\")\n",
    "    model.add(LSTM(lstm_units_2))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    # Hiperparámetros para la capa Densa\n",
    "    dense_units = hp.get(\"dense_units\")\n",
    "    model.add(Dense(dense_units, activation='relu'))\n",
    "    \n",
    "    model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "    # Hiperparámetros para el optimizador\n",
    "    learning_rate = hp.get(\"learning_rate\")\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_gloveLstmDo = best_model_2(best_hp_gloveLstmDo)\n",
    "history_gloveLstmDo = best_model_gloveLstmDo.fit(X_train, y_train,\n",
    "             epochs=30,\n",
    "             validation_split=0.2,\n",
    "             batch_size=4093,\n",
    "             callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_model_3(hp):\n",
    "    model = Sequential(name='3LSTM-DO-CNN-Dense')\n",
    "    model.add(Input(shape=(MAX_LEN, 50)))\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    Construye un modelo secuencial con capas LSTM, Dropout, Conv1D y Densa, utilizando los mejores hiperparámetros encontrados.\n",
    "\n",
    "    :param hp: Objeto HyperParameters que contiene los mejores hiperparámetros encontrados.\n",
    "    :type hp: dict\n",
    "    :return: Modelo construido.\n",
    "    :rtype: tensorflow.keras.models.Sequential\n",
    "    \"\"\"\n",
    "    \n",
    "    # Hiperparámetros para la primera capa LSTM\n",
    "    lstm_units_1 = hp.get(\"lstm_units_1\")\n",
    "    model.add(LSTM(lstm_units_1, name='LSTM1', return_sequences=True))\n",
    "    lstm_dropout_1 = hp.get(\"lstm_dropout_1\")\n",
    "    model.add(Dropout(lstm_dropout_1, name='DO1'))\n",
    "\n",
    "    # Hiperparámetros para la segunda capa LSTM\n",
    "    lstm_units_2 = hp.get(\"lstm_units_2\")\n",
    "    model.add(LSTM(lstm_units_2, name='LSTM2', return_sequences=True))\n",
    "    lstm_dropout_2 = hp.get(\"lstm_dropout_2\")\n",
    "    model.add(Dropout(lstm_dropout_2, name='DO2'))\n",
    "\n",
    "    # Hiperparámetros para la tercera capa LSTM\n",
    "    lstm_units_3 = hp.get(\"lstm_units_3\")\n",
    "    model.add(LSTM(lstm_units_3, name='LSTM3', return_sequences=True))\n",
    "    lstm_dropout_3 = hp.get(\"lstm_dropout_3\")\n",
    "    model.add(Dropout(lstm_dropout_3, name='DO3'))\n",
    "\n",
    "    # Hiperparámetros para la capa Conv1D\n",
    "    conv_filters = hp.get(\"conv_filters\")\n",
    "    model.add(Conv1D(conv_filters, kernel_size=3, strides=1, padding='same', activation='relu'))\n",
    "    \n",
    "    model.add(MaxPooling1D(pool_size=3, strides=2, padding='same'))\n",
    "\n",
    "    cnn_dropout = hp.get(\"cnn_dropout\")\n",
    "    model.add(Dropout(cnn_dropout))\n",
    "    \n",
    "    model.add(Flatten(name='F1'))\n",
    "\n",
    "    # Hiperparámetros para la capa Densa\n",
    "    dense_units = hp.get(\"dense_units\")\n",
    "    model.add(Dense(dense_units, activation='relu'))\n",
    "    \n",
    "    model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "    # Hiperparámetros para el optimizador\n",
    "    learning_rate = hp.get(\"learning_rate\")\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_gloveLstmDoCnn = best_model_3(best_hp_gloveLstmDoCnn)\n",
    "history_gloveLstmDoCnn = best_model_gloveLstmDoCnn.fit(X_train, y_train,\n",
    "             epochs=30,\n",
    "             validation_split=0.2,\n",
    "             batch_size=4093,\n",
    "             callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Glove LSTM\")\n",
    "scoreGloveLstm = best_model_gloveLstm.evaluate(X_test, y_test, verbose=1)\n",
    "print('Test Accuracy:', scoreGloveLstm[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Glove LSTM Do\")\n",
    "scoreGloveLstmDo = best_model_gloveLstmDo.evaluate(X_test, y_test, verbose=1)\n",
    "print('Test Accuracy:', scoreGloveLstmDo[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Glove LSTM Do Cnn\")\n",
    "scoreGloveLstmDoCnn = best_model_gloveLstmDoCnn.evaluate(X_test, y_test, verbose=1)\n",
    "print('Test Accuracy:', scoreGloveLstmDoCnn[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Glove LSTM\")\n",
    "best_model_gloveLstm.save('./saved/GloveLSTM.h5')\n",
    "best_model_gloveLstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Glove LSTM Do\")\n",
    "best_model_gloveLstmDo.save('./saved/GloveLSTMDo.h5')\n",
    "best_model_gloveLstmDo.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Glove LSTM Do Cnn\")\n",
    "best_model_gloveLstmDoCnn.save('./saved/GloveLSTMDoCNN.h5')\n",
    "best_model_gloveLstmDoCnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graficas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Val vs Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def plot_val_vs_loss(history):\n",
    "    \n",
    "    \"\"\"\n",
    "    Grafica las curvas de pérdida y precisión (accuracy) durante el entrenamiento y la validación.\n",
    "\n",
    "    :param history: Historial del entrenamiento que contiene la información de pérdida y precisión.\n",
    "    :type history: tensorflow.python.keras.callbacks.History\n",
    "    \"\"\"\n",
    "    \n",
    "    num_epochs = len(history.epoch)\n",
    "\n",
    "    epochs = [i for i in range(num_epochs)]\n",
    "    fig , ax = plt.subplots(1,2)\n",
    "    train_acc = history.history['accuracy']\n",
    "    train_loss = history.history['loss']\n",
    "    test_acc = history.history['val_accuracy']\n",
    "    test_loss = history.history['val_loss']\n",
    "\n",
    "    fig.set_size_inches(20,6)\n",
    "    ax[0].plot(epochs , train_loss , label = 'Training Loss')\n",
    "    ax[0].plot(epochs , test_loss , label = 'Testing Loss')\n",
    "    ax[0].set_title('Training & Testing Loss')\n",
    "    ax[0].legend()\n",
    "    ax[0].set_xlabel(\"Epochs\")\n",
    "\n",
    "    ax[1].plot(epochs , train_acc , label = 'Training Accuracy')\n",
    "    ax[1].plot(epochs , test_acc , label = 'Testing Accuracy')\n",
    "    ax[1].set_title('Training & Testing Accuracy')\n",
    "    ax[1].legend()\n",
    "    ax[1].set_xlabel(\"Epochs\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_val_vs_loss(history_gloveLstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_val_vs_loss(history_gloveLstmDo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_val_vs_loss(history_gloveLstmDoCnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_GloveLstm = best_model_gloveLstm.predict(X_test)\n",
    "y_pred_GloveLstmDo = best_model_gloveLstmDo.predict(X_test)\n",
    "y_pred_GloveLstmDoCnn = best_model_gloveLstmDoCnn.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metricas de regresion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo Glove LSTM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SSE \n",
    "sse_gloveLstm = np.sum((y_pred_GloveLstm - y_test) ** 2)\n",
    "\n",
    "# SSR \n",
    "ssr_gloveLstm = np.sum((y_pred_GloveLstm - np.mean(y_test)) ** 2)\n",
    "\n",
    "# SSTO \n",
    "ssto_gloveLstm = np.sum((y_test - np.mean(y_test)) ** 2)\n",
    "\n",
    "# MAE \n",
    "mae_gloveLstm = np.mean(np.abs(y_pred_GloveLstm - y_test))\n",
    "\n",
    "# MSE \n",
    "mse_gloveLstm = np.mean((y_pred_GloveLstm - y_test) ** 2)\n",
    "\n",
    "# RMSE \n",
    "rmse_gloveLstm = np.sqrt(mse_gloveLstm)\n",
    "\n",
    "# RMSLE \n",
    "rmsle_gloveLstm = np.sqrt(np.mean((np.log1p(y_pred_GloveLstm) - np.log1p(y_test)) ** 2))\n",
    "\n",
    "# R2 \n",
    "r2_gloveLstm = 1 - (sse_gloveLstm / ssto_gloveLstm)\n",
    "\n",
    "# R2 Ajustado \n",
    "n_gloveLstm = len(y_test)\n",
    "p_gloveLstm = len(y_pred_GloveLstm)\n",
    "adjusted_r2_gloveLstm = 1 - (1 - r2_gloveLstm) * (n_gloveLstm - 1) / (n_gloveLstm - p_gloveLstm - 1)\n",
    "\n",
    "# Varianza de pred\n",
    "variance_pred_gloveLstm = np.var(y_pred_GloveLstm)\n",
    "\n",
    "# Varianza de target\n",
    "variance_target_gloveLstm = np.var(y_test)\n",
    "\n",
    "# Cálculo del AIC \n",
    "n_samples_gloveLstm = len(y_test)\n",
    "residuals_gloveLstm = y_pred_GloveLstm - y_test\n",
    "rss_gloveLstm = np.sum(residuals_gloveLstm ** 2)\n",
    "k_gloveLstm = p_gloveLstm + 1  # número de parámetros incluyendo el término constante\n",
    "aic_gloveLstm = n_samples_gloveLstm * np.log(rss_gloveLstm / n_samples_gloveLstm) + 2 * k_gloveLstm\n",
    "\n",
    "# Cálculo del BIC \n",
    "bic_gloveLstm = n_samples_gloveLstm * np.log(rss_gloveLstm / n_samples_gloveLstm) + k_gloveLstm * np.log(n_samples_gloveLstm)\n",
    "\n",
    "print(\"SSE - Glove LSTM:\", sse_gloveLstm)\n",
    "print(\"SSR - Glove LSTM:\", ssr_gloveLstm)\n",
    "print(\"SSTO - Glove LSTM:\", ssto_gloveLstm)\n",
    "print(\"MAE - Glove LSTM:\", mae_gloveLstm)\n",
    "print(\"MSE - Glove LSTM:\", mse_gloveLstm)\n",
    "print(\"RMSE - Glove LSTM:\", rmse_gloveLstm)\n",
    "print(\"RMSLE - Glove LSTM:\", rmsle_gloveLstm)\n",
    "print(\"R2 - Glove LSTM:\", r2_gloveLstm)\n",
    "print(\"Adjusted R2 - Glove LSTM:\", adjusted_r2_gloveLstm)\n",
    "print(\"Varianza Predicciones - Glove LSTM:\", variance_pred_gloveLstm)\n",
    "print(\"Varianza Objetivo - Glove LSTM:\", variance_target_gloveLstm)\n",
    "print(\"AIC - Glove LSTM:\", aic_gloveLstm)\n",
    "print(\"BIC - Glove LSTM:\", bic_gloveLstm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo GloveLstmDO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SSE \n",
    "sse_gloveLstmDo = np.sum((y_pred_GloveLstmDo - y_test) ** 2)\n",
    "\n",
    "# SSR \n",
    "ssr_gloveLstmDo = np.sum((y_pred_GloveLstmDo - np.mean(y_test)) ** 2)\n",
    "\n",
    "# SSTO \n",
    "ssto_gloveLstmDo = np.sum((y_test - np.mean(y_test)) ** 2)\n",
    "\n",
    "# MAE \n",
    "mae_gloveLstmDo = np.mean(np.abs(y_pred_GloveLstmDo - y_test))\n",
    "\n",
    "# MSE \n",
    "mse_gloveLstmDo = np.mean((y_pred_GloveLstmDo - y_test) ** 2)\n",
    "\n",
    "# RMSE \n",
    "rmse_gloveLstmDo = np.sqrt(mse_gloveLstmDo)\n",
    "\n",
    "# RMSLE \n",
    "rmsle_gloveLstmDo = np.sqrt(np.mean((np.log1p(y_pred_GloveLstmDo) - np.log1p(y_test)) ** 2))\n",
    "\n",
    "# R2 \n",
    "r2_gloveLstmDo = 1 - (sse_gloveLstmDo / ssto_gloveLstmDo)\n",
    "\n",
    "# R2 Ajustado \n",
    "n_gloveLstmDo = len(y_test)\n",
    "p_gloveLstmDo = len(y_pred_GloveLstmDo)\n",
    "adjusted_r2_gloveLstmDo = 1 - (1 - r2_gloveLstmDo) * (n_gloveLstmDo - 1) / (n_gloveLstmDo - p_gloveLstmDo - 1)\n",
    "\n",
    "# Varianza de pred\n",
    "variance_pred_gloveLstmDo = np.var(y_pred_GloveLstmDo)\n",
    "\n",
    "# Varianza de target\n",
    "variance_target_gloveLstmDo = np.var(y_test)\n",
    "\n",
    "# Cálculo del AIC \n",
    "n_samples_gloveLstmDo = len(y_test)\n",
    "residuals_gloveLstmDo = y_pred_GloveLstmDo - y_test\n",
    "rss_gloveLstmDo = np.sum(residuals_gloveLstmDo ** 2)\n",
    "k_gloveLstmDo = p_gloveLstmDo + 1  # número de parámetros incluyendo el término constante\n",
    "aic_gloveLstmDo = n_samples_gloveLstmDo * np.log(rss_gloveLstmDo / n_samples_gloveLstmDo) + 2 * k_gloveLstmDo\n",
    "\n",
    "# Cálculo del BIC \n",
    "bic_gloveLstmDo = n_samples_gloveLstmDo * np.log(rss_gloveLstmDo / n_samples_gloveLstmDo) + k_gloveLstmDo * np.log(n_samples_gloveLstmDo)\n",
    "\n",
    "print(\"SSE - Glove LSTM Do:\", sse_gloveLstmDo)\n",
    "print(\"SSR - Glove LSTM Do:\", ssr_gloveLstmDo)\n",
    "print(\"SSTO - Glove LSTM Do:\", ssto_gloveLstmDo)\n",
    "print(\"MAE - Glove LSTM Do:\", mae_gloveLstmDo)\n",
    "print(\"MSE - Glove LSTM Do:\", mse_gloveLstmDo)\n",
    "print(\"RMSE - Glove LSTM Do:\", rmse_gloveLstmDo)\n",
    "print(\"RMSLE - Glove LSTM Do:\", rmsle_gloveLstmDo)\n",
    "print(\"R2 - Glove LSTM Do:\", r2_gloveLstmDo)\n",
    "print(\"Adjusted R2 - Glove LSTM Do:\", adjusted_r2_gloveLstmDo)\n",
    "print(\"Varianza Predicciones - Glove LSTM Do:\", variance_pred_gloveLstmDo)\n",
    "print(\"Varianza Objetivo - Glove LSTM Do:\", variance_target_gloveLstmDo)\n",
    "print(\"AIC - Glove LSTM Do:\", aic_gloveLstmDo)\n",
    "print(\"BIC - Glove LSTM Do:\", bic_gloveLstmDo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo GloveLstmDoCnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SSE \n",
    "sse_gloveLstmDoCnn = np.sum((y_pred_GloveLstmDoCnn - y_test) ** 2)\n",
    "\n",
    "# SSR \n",
    "ssr_gloveLstmDoCnn = np.sum((y_pred_GloveLstmDoCnn - np.mean(y_test)) ** 2)\n",
    "\n",
    "# SSTO \n",
    "ssto_gloveLstmDoCnn = np.sum((y_test - np.mean(y_test)) ** 2)\n",
    "\n",
    "# MAE\n",
    "mae_gloveLstmDoCnn = np.mean(np.abs(y_pred_GloveLstmDoCnn - y_test))\n",
    "\n",
    "# MSE\n",
    "mse_gloveLstmDoCnn = np.mean((y_pred_GloveLstmDoCnn - y_test) ** 2)\n",
    "\n",
    "# RMSE \n",
    "rmse_gloveLstmDoCnn = np.sqrt(mse_gloveLstmDoCnn)\n",
    "\n",
    "# RMSLE \n",
    "rmsle_gloveLstmDoCnn = np.sqrt(np.mean((np.log1p(y_pred_GloveLstmDoCnn) - np.log1p(y_test)) ** 2))\n",
    "\n",
    "# R2 \n",
    "r2_gloveLstmDoCnn = 1 - (sse_gloveLstmDoCnn / ssto_gloveLstmDoCnn)\n",
    "\n",
    "# R2 Ajustado \n",
    "n_gloveLstmDoCnn = len(y_test)\n",
    "p_gloveLstmDoCnn = len(y_pred_GloveLstmDoCnn)\n",
    "adjusted_r2_gloveLstmDoCnn = 1 - (1 - r2_gloveLstmDoCnn) * (n_gloveLstmDoCnn - 1) / (n_gloveLstmDoCnn - p_gloveLstmDoCnn - 1)\n",
    "\n",
    "# Varianza de pred\n",
    "variance_pred_gloveLstmDoCnn = np.var(y_pred_GloveLstmDoCnn)\n",
    "\n",
    "# Varianza de target\n",
    "variance_target_gloveLstmDoCnn = np.var(y_test)\n",
    "\n",
    "# Cálculo del AIC \n",
    "n_samples_gloveLstmDoCnn = len(y_test)\n",
    "residuals_gloveLstmDoCnn = y_pred_GloveLstmDoCnn - y_test\n",
    "rss_gloveLstmDoCnn = np.sum(residuals_gloveLstmDoCnn ** 2)\n",
    "k_gloveLstmDoCnn = p_gloveLstmDoCnn + 1  # número de parámetros incluyendo el término constante\n",
    "aic_gloveLstmDoCnn = n_samples_gloveLstmDoCnn * np.log(rss_gloveLstmDoCnn / n_samples_gloveLstmDoCnn) + 2 * k_gloveLstmDoCnn\n",
    "\n",
    "# Cálculo del BIC \n",
    "bic_gloveLstmDoCnn = n_samples_gloveLstmDoCnn * np.log(rss_gloveLstmDoCnn / n_samples_gloveLstmDoCnn) + k_gloveLstmDoCnn * np.log(n_samples_gloveLstmDoCnn)\n",
    "\n",
    "print(\"SSE - Glove LSTM Do Cnn:\", sse_gloveLstmDoCnn)\n",
    "print(\"SSR - Glove LSTM Do Cnn:\", ssr_gloveLstmDoCnn)\n",
    "print(\"SSTO - Glove LSTM Do Cnn:\", ssto_gloveLstmDoCnn)\n",
    "print(\"MAE - Glove LSTM Do Cnn:\", mae_gloveLstmDoCnn)\n",
    "print(\"MSE - Glove LSTM Do Cnn:\", mse_gloveLstmDoCnn)\n",
    "print(\"RMSE - Glove LSTM Do Cnn:\", rmse_gloveLstmDoCnn)\n",
    "print(\"RMSLE - Glove LSTM Do Cnn:\", rmsle_gloveLstmDoCnn)\n",
    "print(\"R2 - Glove LSTM Do Cnn:\", r2_gloveLstmDoCnn)\n",
    "print(\"Adjusted R2 - Glove LSTM Do Cnn:\", adjusted_r2_gloveLstmDoCnn)\n",
    "print(\"Varianza Predicciones - Glove LSTM Do Cnn:\", variance_pred_gloveLstmDoCnn)\n",
    "print(\"Varianza Objetivo - Glove LSTM Do Cnn:\", variance_target_gloveLstmDoCnn)\n",
    "print(\"AIC - Glove LSTM Do Cnn:\", aic_gloveLstmDoCnn)\n",
    "print(\"BIC - Glove LSTM Do Cnn:\", bic_gloveLstmDoCnn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_GloveLstm[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_np =  np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_heatmap(y_pred, y_test, labels):\n",
    "    \n",
    "     \"\"\"\n",
    "    Grafica un heatmap de la matriz de confusión.\n",
    "\n",
    "    :param y_pred: Predicciones del modelo.\n",
    "    :type y_pred: numpy.ndarray\n",
    "    :param y_test: Etiquetas verdaderas.\n",
    "    :type y_test: numpy.ndarray\n",
    "    :param labels: Etiquetas de las clases.\n",
    "    :type labels: list[str]\n",
    "    \"\"\"\n",
    "        \n",
    "    # Realizar predicciones\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    y_true_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "    # Calcular la matriz de confusión\n",
    "    cm = confusion_matrix(y_true_classes, y_pred_classes)\n",
    "\n",
    "    # Crear el heatmap utilizando Seaborn\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='coolwarm', cbar=False, xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()\n",
    "\n",
    "labels = ['Clase 1', 'Clase 2', 'Clase 3', 'Clase 4', 'Clase 5', 'Clase 6', 'Clase 7']\n",
    "plot_heatmap(y_pred_GloveLstm, y_test_np, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_heatmap(y_pred_GloveLstmDo, y_test_np, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_heatmap(y_pred_GloveLstmDoCnn, y_test_np, labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
